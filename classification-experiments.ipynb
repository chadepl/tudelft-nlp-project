{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,ShuffleSplit,learning_curve,KFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import copy\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "size_set = \"small\"\n",
    "pd.set_option('precision', 4)\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "goal_metrics = ['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.44      0.53      0.48       201\n",
      "no-clickbait       0.75      0.68      0.71       414\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.60      0.60      0.60       615\n",
      "weighted avg       0.65      0.63      0.64       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[107  94]\n",
      " [134 280]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.50      0.44      0.47       201\n",
      "no-clickbait       0.74      0.78      0.76       414\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.62      0.61      0.62       615\n",
      "weighted avg       0.66      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 89 112]\n",
      " [ 90 324]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.00      0.00      0.00       201\n",
      "no-clickbait       0.67      1.00      0.80       414\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.34      0.50      0.40       615\n",
      "weighted avg       0.45      0.67      0.54       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  0 201]\n",
      " [  0 414]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.33      0.28      0.30       201\n",
      "no-clickbait       0.67      0.72      0.70       414\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       615\n",
      "   macro avg       0.50      0.50      0.50       615\n",
      "weighted avg       0.56      0.58      0.57       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 56 145]\n",
      " [115 299]]\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifiers = train_classifiers(X_train[:,0:97],y_train)\n",
    "report_classifiers(classifiers,X_test,y_test,thing,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature sets and get them ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the features that we created in the preprocessing and feature extraction notebook. The schema of this file is the following: ADD SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = None,None,None,None\n",
    "if(size_set == \"small\"):   \n",
    "    dataset= \"feature_set_small.csv\"\n",
    "    feature = \"labels_set_small.csv\"  \n",
    "else:\n",
    "    dataset = \"feature_set_large.csv\"\n",
    "    feature = \"labels_set_large.csv\"\n",
    "\n",
    "data = pd.read_csv(dataset).fillna(0)\n",
    "labels = pd.read_csv(feature)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.values, labels.values[:,0], test_size=0.25)\n",
    "names = list(data)\n",
    "ratio = float(np.sum(y_train == 'no-clickbait')) / np.sum(y_train == 'clickbait')\n",
    "classifier_names = [\"AdaBoost\",\"XGBoost\",\"Naive Bayes\", \"KNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.01600000e+03, 6.09079439e+17, 9.00000000e+00, ...,\n",
       "        9.16324386e-01, 7.03102134e-01, 0.00000000e+00],\n",
       "       [1.26400000e+03, 6.09935186e+17, 6.60000000e+01, ...,\n",
       "        1.00000000e+00, 8.61285668e-01, 0.00000000e+00],\n",
       "       [1.75000000e+02, 6.08365726e+17, 7.30000000e+01, ...,\n",
       "        6.44805116e-01, 7.60151032e-01, 4.91973680e-01],\n",
       "       ...,\n",
       "       [1.06600000e+03, 6.09888859e+17, 7.90000000e+01, ...,\n",
       "        8.27623542e-01, 7.75745971e-01, 6.00597101e-01],\n",
       "       [1.68000000e+03, 6.09508859e+17, 3.00000000e+01, ...,\n",
       "        8.80061442e-01, 6.09443783e-01, 0.00000000e+00],\n",
       "       [2.16500000e+03, 6.09556314e+17, 4.70000000e+01, ...,\n",
       "        1.00000000e+00, 7.96602916e-01, 5.63642509e-01]])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution of classes:  2.227034120734908\n",
      "Train distribution of classes:  2.28698752228164\n",
      "Test distribution of classes:  2.0597014925373136\n"
     ]
    }
   ],
   "source": [
    "np.sum(y_train == 'clickbait')/y_train.shape[0]\n",
    "\n",
    "print(\"Original distribution of classes: \",float(np.sum(labels.values[:,0] == 'no-clickbait')) / np.sum(labels.values[:,0] == 'clickbait'))\n",
    "print(\"Train distribution of classes: \",float(np.sum(y_train == 'no-clickbait')) / np.sum(y_train == 'clickbait'))\n",
    "print(\"Test distribution of classes: \",float(np.sum(y_test == 'no-clickbait')) / np.sum(y_test == 'clickbait'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original clickbait proportion:  0.3098820658804392\n",
      "Train set Confidence interval ( 0.32522945914077334 0.2832304107073828 )\n",
      "Test set Confidence interval ( 0.3639008910412912 0.2897576455440747 )\n"
     ]
    }
   ],
   "source": [
    "p_original = np.sum(labels.values[:,0] == 'clickbait')/np.sum(labels.values[:,0].shape[0])\n",
    "p_train = np.sum(y_train == 'clickbait')/y_train.shape[0]\n",
    "p_test = np.sum(y_test == 'clickbait')/y_test.shape[0]\n",
    "me_train = 1.96*np.sqrt((p_train*(1-p_train))/y_train.shape[0])\n",
    "me_test = 1.96*np.sqrt((p_test*(1-p_test))/y_test.shape[0])\n",
    "print(\"Original clickbait proportion: \", str(p_original))                                                           \n",
    "print(\"Train set Confidence interval\",'( '+str(p_train+me_train),str(p_train-me_train)+' )')\n",
    "print(\"Test set Confidence interval\",'( '+str(p_test+me_test),str(p_test-me_test)+' )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.524847092759644\n",
      "0.9088154392538469\n"
     ]
    }
   ],
   "source": [
    "ztr = (p_train - p_original)/np.sqrt((p_original*(1-p_original))/y_train.shape[0])\n",
    "zt = (p_test - p_original)/np.sqrt((p_original*(1-p_original))/y_test.shape[0])\n",
    "print(ztr)\n",
    "print(zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_features(X_train,X_test,type_scale=\"standard\"):\n",
    "    train,test = None,None\n",
    "    if(type_scale == \"standard\"):\n",
    "        scaler = StandardScaler()\n",
    "        train =  copy.deepcopy(scaler.fit_transform(X_train))\n",
    "        test = copy.deepcopy(scaler.transform(X_test))\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        train = copy.deepcopy(scaler.fit_transform(X_train))\n",
    "        test = copy.deepcopy(scaler.transform(X_test))\n",
    "    return [train,test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 featRatioCharTargetDescription_TargetKeywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0, 32])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(names[0],names[32])\n",
    "\n",
    "np.argwhere(a.get_support()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(names,mutual_info_classif(X_train,y_train)))\n",
    "feature_importance_table = pd.DataFrame(feature_importance,columns=[\"feature\",\"importance\"]) \n",
    "feature_importance_table = feature_importance_table.sort_values(\"importance\",ascending=False)\n",
    "feature_importance_table['index'] = range(1, len(feature_importance_table) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "                                         feature &  importance &  index \\\\\n",
      "\\midrule\n",
      " featRatioCharTargetDescription\\_TargetParagraphs &      0.0389 &      1 \\\\\n",
      "                                featCountPOS\\_WRB &      0.0387 &      2 \\\\\n",
      "                                featCountPOS\\_NNP &      0.0386 &      3 \\\\\n",
      "   featRatioCharTargetDescription\\_TargetKeywords &      0.0383 &      4 \\\\\n",
      "                         featCountPOS\\_WRB\\_NNS\\_RB &      0.0374 &      5 \\\\\n",
      "            featRatioCharPostText\\_TargetKeywords &      0.0329 &      6 \\\\\n",
      "                           featCountPOS\\_DT\\_JJ\\_RB &      0.0310 &      7 \\\\\n",
      "                             featCountPOS\\_NNP\\_NN &      0.0296 &      8 \\\\\n",
      "                         featCountPOS\\_RB\\_VBZ\\_NNP &      0.0281 &      9 \\\\\n",
      "                          featCountPOS\\_NN\\_IN\\_NNP &      0.0279 &     10 \\\\\n",
      "         featRatioCharPostText\\_TargetDescription &      0.0278 &     11 \\\\\n",
      "                    featNumCharTargetDescription &      0.0269 &     12 \\\\\n",
      "                      featNumFormalWordsPostText &      0.0264 &     13 \\\\\n",
      "                         featCountPOS\\_RB\\_VBZ\\_NNS &      0.0263 &     14 \\\\\n",
      "          featDiffWordsPostText\\_TargetParagraphs &      0.0261 &     15 \\\\\n",
      "                            featNumWordsPostText &      0.0260 &     16 \\\\\n",
      "         featRatioCharTargetKeywords\\_TargetTitle &      0.0257 &     17 \\\\\n",
      "         featRatioWordsPostText\\_TargetParagraphs &      0.0257 &     18 \\\\\n",
      "            featRatioCharPostText\\_TargetCaptions &      0.0256 &     19 \\\\\n",
      "                           featCountPOS\\_DT\\_JJ\\_NN &      0.0234 &     20 \\\\\n",
      "   featRatioCharTargetCaptions\\_TargetDescription &      0.0231 &     21 \\\\\n",
      "          featDiffCharTargetKeywords\\_TargetTitle &      0.0228 &     22 \\\\\n",
      "                         featCountPOS\\_VBZ\\_NNS\\_JJ &      0.0226 &     23 \\\\\n",
      "     featRatioWordsTargetDescription\\_TargetTitle &      0.0223 &     24 \\\\\n",
      "                         featCountPOS\\_NNS\\_NNP\\_IN &      0.0222 &     25 \\\\\n",
      " featDiffWordsTargetDescription\\_TargetParagraphs &      0.0219 &     26 \\\\\n",
      "                         featCountPOS\\_PRP\\_NNP\\_JJ &      0.0219 &     27 \\\\\n",
      "             featDiffCharPostText\\_TargetKeywords &      0.0219 &     28 \\\\\n",
      "                             featCountPOS\\_PRP\\_RB &      0.0215 &     29 \\\\\n",
      "      featRatioWordsTargetParagraphs\\_TargetTitle &      0.0211 &     30 \\\\\n",
      "                             featCountPOS\\_DT\\_NNP &      0.0203 &     31 \\\\\n",
      "     featDiffCharTargetKeywords\\_TargetParagraphs &      0.0198 &     32 \\\\\n",
      "                                featCountPOS\\_VBZ &      0.0198 &     33 \\\\\n",
      "                         featCountPOS\\_JJ\\_NNP\\_NNS &      0.0197 &     34 \\\\\n",
      "                              featCountPOS\\_DT\\_JJ &      0.0197 &     35 \\\\\n",
      "                     featNumCharTargetParagraphs &      0.0196 &     36 \\\\\n",
      "           featRatioWordsPostText\\_TargetKeywords &      0.0193 &     37 \\\\\n",
      "                         featCountPOS\\_NN\\_VBZ\\_NNP &      0.0192 &     38 \\\\\n",
      "          featPercentFormalWordsTargetParagraphs &      0.0192 &     39 \\\\\n",
      "                             featCountPOS\\_RBS\\_NN &      0.0189 &     40 \\\\\n",
      "          featRatioCharPostText\\_TargetParagraphs &      0.0187 &     41 \\\\\n",
      "       featPercentInformalWordsTargetDescription &      0.0185 &     42 \\\\\n",
      "                                featCountPOS\\_NNS &      0.0183 &     43 \\\\\n",
      "                           featCountPOS\\_DT\\_NN\\_IN &      0.0181 &     44 \\\\\n",
      "                         featCountPOS\\_VBZ\\_NNP\\_RB &      0.0181 &     45 \\\\\n",
      "                           featCountPOS\\_DT\\_RB\\_JJ &      0.0180 &     46 \\\\\n",
      "                             featNumCharPostText &      0.0180 &     47 \\\\\n",
      "                         featCountPOS\\_VBZ\\_JJ\\_NNS &      0.0179 &     48 \\\\\n",
      "       featRatioCharTargetParagraphs\\_TargetTitle &      0.0177 &     49 \\\\\n",
      "            featDiffWordsPostText\\_TargetCaptions &      0.0176 &     50 \\\\\n",
      "    featRatioCharTargetCaptions\\_TargetParagraphs &      0.0172 &     51 \\\\\n",
      "   featRatioWordsTargetCaptions\\_TargetParagraphs &      0.0171 &     52 \\\\\n",
      "    featDiffCharTargetDescription\\_TargetKeywords &      0.0168 &     53 \\\\\n",
      "                           featCountPOS\\_JJ\\_NN\\_IN &      0.0168 &     54 \\\\\n",
      "                                 featCountPOS\\_WP &      0.0168 &     55 \\\\\n",
      "           featRatioWordsPostText\\_TargetCaptions &      0.0167 &     56 \\\\\n",
      "            featDiffWordsPostText\\_TargetKeywords &      0.0164 &     57 \\\\\n",
      "                             featCountPOS\\_WRB\\_RB &      0.0163 &     58 \\\\\n",
      "                             featCountPOS\\_IN\\_PRP &      0.0156 &     59 \\\\\n",
      "                          featCountPOS\\_NN\\_RB\\_NNS &      0.0150 &     60 \\\\\n",
      "                             featCountPOS\\_WRB\\_JJ &      0.0150 &     61 \\\\\n",
      "                              featCountPOS\\_IN\\_DT &      0.0149 &     62 \\\\\n",
      "                        featCountPOS\\_NNP\\_VBZ\\_NNS &      0.0149 &     63 \\\\\n",
      "                          featCountPOS\\_WRB\\_JJ\\_NN &      0.0148 &     64 \\\\\n",
      "         featRatioCharTargetCaptions\\_TargetTitle &      0.0147 &     65 \\\\\n",
      "      featRatioCharTargetDescription\\_TargetTitle &      0.0147 &     66 \\\\\n",
      "      featRatioCharTargetCaptions\\_TargetKeywords &      0.0147 &     67 \\\\\n",
      "                         featCountPOS\\_NNP\\_NN\\_WRB &      0.0147 &     68 \\\\\n",
      "                          featCountPOS\\_NN\\_JJ\\_VBZ &      0.0146 &     69 \\\\\n",
      "                             featCountPOS\\_VBZ\\_IN &      0.0145 &     70 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance_table.loc[feature_importance_table['index'] <= 70].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selector(X,y,number_feat):\n",
    "    return SelectKBest(mutual_info_classif, k=number_feat).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_curves(X_train,y_train,X_test,y_test,n):\n",
    "    performances = np.zeros((4,X_train.shape[1]))\n",
    "    for i in range(n):   \n",
    "        selector = get_selector(X_train,y_train,i+1)\n",
    "        np.argwhere(selector.get_support()).flatten()\n",
    "        X_new_train = selector.transform(X_train)\n",
    "        print(\"SHAPE SELECTED FEATURES\",X_new_train.shape)\n",
    "        classifiers = train_classifiers(X_new_train,y_train)\n",
    "        X_new_test = selector.transform(X_test)\n",
    "        for enum, classifier in enumerate(classifiers):\n",
    "            y_true, y_pred = y_test, classifier.predict(X_new_test)\n",
    "            performances[enum,i] = f1_score(y_true, y_pred, average='weighted')    \n",
    "    return performances;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params(classifier,param_grid,X,y):\n",
    "    print(type(classifier).__name__,thing)\n",
    "    for score in goal_metrics:\n",
    "        print(\"# Tuning hyper-parameters for %s\" %score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(classifier, param_grid, cv=5,\n",
    "                           scoring='%s_macro' % score)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        #          % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_adaboost = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                  \"base_estimator__splitter\" :   [\"best\"],\n",
    "                  \"base_estimator__class_weight\": [\"balanced\",None],\n",
    "                  \"n_estimators\": [10,20,30,40,50]\n",
    "                 }\n",
    "\n",
    "grid_xgboost = { \n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], \n",
    "              'max_depth': [5,6,7],\n",
    "              'booster': ['gbtree'],\n",
    "              'n_estimators': [20,30,40],\n",
    "              'scale_pos_weight':[1,ratio, 1/ratio]}\n",
    "grid_knn = {'n_neighbors':[2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "#best adaboost \n",
    "#{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
    "#best xgboost\n",
    "#{'booster': 'gbtree', 'lambda': 0, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.4623314829500396}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(thing_tried,classifier, X_test,y_test,initial):\n",
    "        print(\"STEP IN PIPELINE: \",thing_tried)\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        if(initial==1):\n",
    "            y_true, y_pred = y_test, classifier.predict(X_test[:,0:97])\n",
    "        else:\n",
    "            y_true, y_pred = y_test, classifier.predict(X_test)\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "        % (classifier, metrics.classification_report(y_true, y_pred)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_true, y_pred))\n",
    "        print(\"--------------------------\")\n",
    "        return [y_true, y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_classifiers(classifiers,X_test,y_test,step,initial):\n",
    "    for c in classifiers:\n",
    "        classification_report(step,c,X_test,y_test,initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifiers(X_train,y_train):\n",
    "    print(\"HYPER PARAMETER TUNING  *********************\")\n",
    "    adaboost_c = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,))\n",
    "    xgb_c = xgb.XGBClassifier()\n",
    "    gnb_c = GaussianNB()\n",
    "    knn_c = KNeighborsClassifier()\n",
    "    \n",
    "    adaboost_c = best_params(adaboost_c,grid_adaboost,X_train,y_train)\n",
    "    xgb_c = best_params(xgb_c,grid_xgboost,X_train,y_train)   \n",
    "    gnb_c.fit(X_train,y_train)\n",
    "    knn_c = best_params(knn_c,grid_knn,X_train,y_train)\n",
    "    classifiers =[adaboost_c,xgb_c,gnb_c,knn_c]\n",
    "    print(\"END HYPERPARAMETER TUNING  *********************\")\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.62      0.42      0.50       208\n",
      "no-clickbait       0.74      0.87      0.80       407\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       615\n",
      "   macro avg       0.68      0.64      0.65       615\n",
      "weighted avg       0.70      0.72      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 87 121]\n",
      " [ 54 353]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.56      0.56      0.56       208\n",
      "no-clickbait       0.78      0.77      0.77       407\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.67      0.67      0.67       615\n",
      "weighted avg       0.70      0.70      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[117  91]\n",
      " [ 92 315]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.00      0.00      0.00       208\n",
      "no-clickbait       0.66      1.00      0.80       407\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       615\n",
      "   macro avg       0.33      0.50      0.40       615\n",
      "weighted avg       0.44      0.66      0.53       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  0 208]\n",
      " [  0 407]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.36      0.35      0.35       208\n",
      "no-clickbait       0.67      0.68      0.68       407\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       615\n",
      "   macro avg       0.51      0.51      0.51       615\n",
      "weighted avg       0.57      0.57      0.57       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 72 136]\n",
      " [129 278]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.51      0.58      0.54       208\n",
      "no-clickbait       0.77      0.72      0.74       407\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.64      0.65      0.64       615\n",
      "weighted avg       0.68      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[120  88]\n",
      " [115 292]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.62      0.57      0.60       208\n",
      "no-clickbait       0.79      0.82      0.81       407\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       615\n",
      "   macro avg       0.71      0.70      0.70       615\n",
      "weighted avg       0.73      0.74      0.74       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[119  89]\n",
      " [ 72 335]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.34      0.92      0.50       208\n",
      "no-clickbait       0.68      0.09      0.16       407\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       615\n",
      "   macro avg       0.51      0.50      0.33       615\n",
      "weighted avg       0.56      0.37      0.27       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191  17]\n",
      " [371  36]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.47      0.42      0.45       208\n",
      "no-clickbait       0.72      0.76      0.74       407\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       615\n",
      "   macro avg       0.60      0.59      0.59       615\n",
      "weighted avg       0.64      0.65      0.64       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 88 120]\n",
      " [ 98 309]]\n",
      "\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.51      0.61      0.55       208\n",
      "no-clickbait       0.78      0.70      0.74       407\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.64      0.65      0.64       615\n",
      "weighted avg       0.69      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[126  82]\n",
      " [122 285]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.56      0.56      0.56       208\n",
      "no-clickbait       0.78      0.77      0.77       407\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.67      0.67      0.67       615\n",
      "weighted avg       0.70      0.70      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[117  91]\n",
      " [ 92 315]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.34      0.91      0.50       208\n",
      "no-clickbait       0.68      0.09      0.16       407\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       615\n",
      "   macro avg       0.51      0.50      0.33       615\n",
      "weighted avg       0.56      0.37      0.28       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  18]\n",
      " [369  38]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.45      0.44      0.44       208\n",
      "no-clickbait       0.72      0.73      0.72       407\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.58      0.58      0.58       615\n",
      "weighted avg       0.63      0.63      0.63       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 91 117]\n",
      " [111 296]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#things_try = ['normal','scale','feture_selection','feature_extraction','threshold']\n",
    "things_try = ['normal','scale']\n",
    "\n",
    "for thing in things_try:\n",
    "    if(thing==\"original\"):\n",
    "        classifiers = train_classifiers(X_train[:,0:97],y_train)\n",
    "        report_classifiers(classifiers,X_test,y_test,thing)\n",
    "        \n",
    "    if(thing == 'normal'):    \n",
    "        classifiers = train_classifiers(X_train,y_train)\n",
    "        report_classifiers(classifiers,X_test,y_test,thing)\n",
    "    if(thing == 'scale'):\n",
    "        type_scale = \"standard\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,\"standard\")\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        report_classifiers(classifiers,new_xtest,y_test,thing+str(type_scale))\n",
    "        \n",
    "        type_scale = \"minmax\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        report_classifiers(classifiers,new_xtest,y_test,thing+str(type_scale))\n",
    "    if(thing==\"feture_selection\"):\n",
    "        #number of featrues to check\n",
    "        n=45\n",
    "        type_scale = \"minmax\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        get_feature_curves(classifiers, new_xtrain,y_train,new_xtest,y_test,n)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare our features with theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 502)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 1)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 2)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 3)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 4)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 5)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 6)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 7)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 8)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 9)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 10)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 11)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 12)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 13)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 14)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 15)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 16)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 17)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 18)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 19)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 20)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 21)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 22)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 23)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 24)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 25)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 5}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 26)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 27)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 28)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 29)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 30)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 31)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 32)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 33)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 34)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 35)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 36)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 37)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 38)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 39)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 40)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 41)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 42)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 43)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 44)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 45)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 46)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 47)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 48)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 49)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 50)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 51)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 52)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 53)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 54)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 55)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 56)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 57)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 58)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 59)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 60)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 61)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 62)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 63)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 64)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 65)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 66)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 67)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 68)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 69)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 70)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 71)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 72)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 73)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 74)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 75)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 76)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 77)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 78)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 79)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 80)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 81)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 82)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 83)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 84)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 85)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 86)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 87)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 88)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 89)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 90)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 91)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 92)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 93)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 94)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 95)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 96)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 97)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 98)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 99)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 100)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 101)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 102)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 103)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 104)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 105)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 106)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 107)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 108)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 109)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 110)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 7}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 111)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 112)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 113)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 114)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 115)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 116)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 117)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 118)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 119)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 120)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 121)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 122)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 123)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 124)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 125)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 126)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 127)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 128)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 129)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 130)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 131)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 132)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 133)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 134)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 135)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 136)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 137)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 138)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 139)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 140)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 141)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 142)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 143)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 144)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 145)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 146)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 147)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 148)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 149)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 150)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 151)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 152)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 153)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 154)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 155)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 156)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 157)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 158)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 159)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 160)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    }
   ],
   "source": [
    "#number of featrues to check\n",
    "n=160\n",
    "type_scale = \"standard\"\n",
    "[new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "performance = get_feature_curves(new_xtrain,y_train,new_xtest,y_test,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=160\n",
    "x_axis = np.arange(n) +1\n",
    "plt.plot(x_axis,performance[0,0:n],'-b') \n",
    "plt.plot(x_axis,performance[1,0:n],'-g') \n",
    "#plt.plot(x_axis,performance[3,0:n],'-k') \n",
    "\n",
    "plt.axvline(x=158,color='r',linestyle='--')\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 160\n",
    "x_axis = np.arange(n) +1\n",
    "avada = moving_average(performance[0,0:n],10)\n",
    "avxg = moving_average(performance[1,0:n],10)\n",
    "avnaive = moving_average(performance[2,0:n],10)\n",
    "avknn = moving_average(performance[3,0:n],10)\n",
    "plt.plot(np.arange(avada.shape[0])+1,avada+0.02,'-b') \n",
    "plt.plot(np.arange(avada.shape[0])+1,avxg+0.02,'-g') \n",
    "#plt.plot(np.arange(avada.shape[0])+1,avnaive+0.01,'-y') \n",
    "plt.plot(np.arange(avada.shape[0])+1,avknn+0.015,'-k') \n",
    "plt.axvline(x=130,color='r',linestyle='--')\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "c_names = [\"AdaBoost\", \"XGBoost\", \"KNN\"]\n",
    "plt.title('Feature curve') \n",
    "plt.legend(c_names, loc='lower right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82, 67, 88, 88])"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(performance,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'adaboost':avada,'XGBoost':avxg,'Naive':avnaive,'KNN':avknn}).to_csv('feature_curves.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.51      0.52      0.51       192\n",
      "no-clickbait       0.78      0.77      0.77       423\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       615\n",
      "   macro avg       0.64      0.64      0.64       615\n",
      "weighted avg       0.69      0.69      0.69       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[100  92]\n",
      " [ 98 325]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.53      0.48      0.50       192\n",
      "no-clickbait       0.77      0.80      0.79       423\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.65      0.64      0.64       615\n",
      "weighted avg       0.70      0.70      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 92 100]\n",
      " [ 83 340]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.31      0.86      0.45       192\n",
      "no-clickbait       0.63      0.11      0.18       423\n",
      "\n",
      "   micro avg       0.34      0.34      0.34       615\n",
      "   macro avg       0.47      0.49      0.32       615\n",
      "weighted avg       0.53      0.34      0.27       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[166  26]\n",
      " [378  45]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.44      0.40      0.42       192\n",
      "no-clickbait       0.74      0.77      0.76       423\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       615\n",
      "   macro avg       0.59      0.58      0.59       615\n",
      "weighted avg       0.65      0.66      0.65       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 76 116]\n",
      " [ 96 327]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "n=502\n",
    "x_train_n = get_selector(new_xtrain,y_train,n).transform(new_xtrain)\n",
    "x_test_n = get_selector(new_xtrain,y_train,n).transform(new_xtest)\n",
    "\n",
    "classifiers = train_classifiers(x_train_n,y_train)\n",
    "report_classifiers(classifiers,x_test_n,y_test,\"PCA\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.80000000000007"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(new_xtrain)\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "var[208] #cumulative sum of variance explained with [n] features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1ff83d30>]"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(30,100.5)\n",
    "plt.style.context('seaborn-whitegrid')\n",
    "plt.axvline(x=250,color='r',linestyle='--',label=\"Cut at 38 features\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 250)\n",
    "pca.fit(new_xtrain)\n",
    "pca_x_train = pca.transform(new_xtrain)\n",
    "pca_x_test = pca.transform(new_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1844, 250)"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    }
   ],
   "source": [
    "classifiers = train_classifiers(pca_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.45      0.51      0.48       192\n",
      "no-clickbait       0.76      0.71      0.74       423\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       615\n",
      "   macro avg       0.60      0.61      0.61       615\n",
      "weighted avg       0.66      0.65      0.65       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 98  94]\n",
      " [122 301]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=7, min_child_weight=1, missing=None, n_estimators=30,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.57      0.37      0.45       192\n",
      "no-clickbait       0.75      0.87      0.81       423\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       615\n",
      "   macro avg       0.66      0.62      0.63       615\n",
      "weighted avg       0.70      0.72      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 71 121]\n",
      " [ 54 369]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.38      0.29      0.33       192\n",
      "no-clickbait       0.71      0.79      0.75       423\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.55      0.54      0.54       615\n",
      "weighted avg       0.61      0.63      0.62       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 55 137]\n",
      " [ 89 334]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.41      0.45      0.43       192\n",
      "no-clickbait       0.74      0.70      0.72       423\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.57      0.58      0.58       615\n",
      "weighted avg       0.64      0.63      0.63       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 87 105]\n",
      " [125 298]]\n",
      "--------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-803-5e605b2aad85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mverdad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpca_x_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PCA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "report_classifiers(classifiers,pca_x_test,y_test,\"PCA\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting bad samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP IN PIPELINE:  ERROR ANALYSOS\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=7, min_child_weight=1, missing=None, n_estimators=30,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.57      0.37      0.45       192\n",
      "no-clickbait       0.75      0.87      0.81       423\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       615\n",
      "   macro avg       0.66      0.62      0.63       615\n",
      "weighted avg       0.70      0.72      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 71 121]\n",
      " [ 54 369]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "y_true_error,y_pred_error = classification_report(\"ERROR ANALYSOS\",classifiers[1],pca_x_test,y_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches = (y_true_error != y_pred_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.argwhere(mismatches == True).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.75000000e+02, 6.08365726e+17, 7.30000000e+01, ...,\n",
       "        6.44805116e-01, 7.60151032e-01, 4.91973680e-01],\n",
       "       [7.49000000e+02, 6.10133347e+17, 5.10000000e+01, ...,\n",
       "        8.48677385e-01, 8.68080164e-01, 8.19127749e-01],\n",
       "       [1.16600000e+03, 6.08257995e+17, 7.30000000e+01, ...,\n",
       "        9.40065797e-01, 8.56701712e-01, 4.24816422e-01],\n",
       "       ...,\n",
       "       [3.41000000e+02, 6.08151562e+17, 5.10000000e+01, ...,\n",
       "        9.70129758e-01, 8.72624831e-01, 6.20791496e-01],\n",
       "       [1.91000000e+03, 6.08670468e+17, 6.00000000e+01, ...,\n",
       "        9.37234383e-01, 8.30898851e-01, 5.73108229e-01],\n",
       "       [1.68000000e+03, 6.09508859e+17, 3.00000000e+01, ...,\n",
       "        8.80061442e-01, 6.09443783e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.X_test[ids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.DataFrame(data=X_test[ids,:],columns=names).to_csv('errors_made.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>featNumCharPostText</th>\n",
       "      <th>featNumCharTargetTitle</th>\n",
       "      <th>featNumCharTargetDescription</th>\n",
       "      <th>featNumCharTargetKeywords</th>\n",
       "      <th>featNumCharTargetCaptions</th>\n",
       "      <th>featNumCharTargetParagraphs</th>\n",
       "      <th>featDiffCharPostText_TargetCaptions</th>\n",
       "      <th>featDiffCharPostText_TargetDescription</th>\n",
       "      <th>...</th>\n",
       "      <th>featCountPOS_WRB_NNP_NN</th>\n",
       "      <th>featCountPOS_WRB_NNP_NNS</th>\n",
       "      <th>featCountPOS_WRB_NNP_VBZ</th>\n",
       "      <th>featCountPOS_WRB_RB_NN</th>\n",
       "      <th>featCountPOS_WRB_VBZ_NNS</th>\n",
       "      <th>featIsNEPresent</th>\n",
       "      <th>featSentiment</th>\n",
       "      <th>featSimilarityPostTextTargetTitle</th>\n",
       "      <th>featSimilarityPostTextTargetParagraphs</th>\n",
       "      <th>featSimilarityPostTextTargetKeywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>608310377143799810</td>\n",
       "      <td>59</td>\n",
       "      <td>87</td>\n",
       "      <td>141</td>\n",
       "      <td>66</td>\n",
       "      <td>2059</td>\n",
       "      <td>2861</td>\n",
       "      <td>2000</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>609297109095972864</td>\n",
       "      <td>75</td>\n",
       "      <td>51</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>1875</td>\n",
       "      <td>309</td>\n",
       "      <td>1800</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>609504474621612032</td>\n",
       "      <td>70</td>\n",
       "      <td>51</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>2903</td>\n",
       "      <td>267</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>609748367049105409</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>94</td>\n",
       "      <td>120</td>\n",
       "      <td>846</td>\n",
       "      <td>8127</td>\n",
       "      <td>788</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>608688782821453825</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>81</td>\n",
       "      <td>47</td>\n",
       "      <td>149</td>\n",
       "      <td>4006</td>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>609551038983475201</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>609447408955719681</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>102</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>4345</td>\n",
       "      <td>74</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>609027430624288769</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>554</td>\n",
       "      <td>56</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>608229011572068352</td>\n",
       "      <td>75</td>\n",
       "      <td>56</td>\n",
       "      <td>101</td>\n",
       "      <td>144</td>\n",
       "      <td>871</td>\n",
       "      <td>252</td>\n",
       "      <td>796</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>609046214554755073</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>3229</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>609738211183751168</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>99</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>1727</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>609762671554990080</td>\n",
       "      <td>38</td>\n",
       "      <td>69</td>\n",
       "      <td>110</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>1525</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>608980783924178945</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>4709</td>\n",
       "      <td>65</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>609998126737395712</td>\n",
       "      <td>70</td>\n",
       "      <td>129</td>\n",
       "      <td>145</td>\n",
       "      <td>17</td>\n",
       "      <td>80</td>\n",
       "      <td>3425</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>608926452596273152</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>115</td>\n",
       "      <td>2488</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>609300383358418945</td>\n",
       "      <td>65</td>\n",
       "      <td>50</td>\n",
       "      <td>95</td>\n",
       "      <td>241</td>\n",
       "      <td>1662</td>\n",
       "      <td>283</td>\n",
       "      <td>1597</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>608875876474851328</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>77</td>\n",
       "      <td>18</td>\n",
       "      <td>144</td>\n",
       "      <td>3132</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>610173844561788928</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>103</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>3140</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>609095030926635011</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>54</td>\n",
       "      <td>1450</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>608576531456249856</td>\n",
       "      <td>48</td>\n",
       "      <td>112</td>\n",
       "      <td>153</td>\n",
       "      <td>80</td>\n",
       "      <td>1688</td>\n",
       "      <td>1221</td>\n",
       "      <td>1640</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>609631563597389824</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>94</td>\n",
       "      <td>54</td>\n",
       "      <td>198</td>\n",
       "      <td>5478</td>\n",
       "      <td>154</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>609372816568360960</td>\n",
       "      <td>86</td>\n",
       "      <td>50</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1129</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>610103181578764291</td>\n",
       "      <td>44</td>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>581</td>\n",
       "      <td>44</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>609425861561962496</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>4212</td>\n",
       "      <td>74</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>608658774769958912</td>\n",
       "      <td>35</td>\n",
       "      <td>52</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>609396415681445888</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>609337131698229249</td>\n",
       "      <td>53</td>\n",
       "      <td>30</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>87180</td>\n",
       "      <td>2371</td>\n",
       "      <td>87127</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>610152342521028609</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>117</td>\n",
       "      <td>224</td>\n",
       "      <td>189</td>\n",
       "      <td>3975</td>\n",
       "      <td>130</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>608315579913629696</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>609417622485139456</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>146</td>\n",
       "      <td>163</td>\n",
       "      <td>257</td>\n",
       "      <td>5773</td>\n",
       "      <td>196</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>2429</td>\n",
       "      <td>608881572834619392</td>\n",
       "      <td>72</td>\n",
       "      <td>48</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2700</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>2430</td>\n",
       "      <td>610096771746930688</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>95</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>2087</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>2431</td>\n",
       "      <td>608289752438177792</td>\n",
       "      <td>87</td>\n",
       "      <td>33</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>162</td>\n",
       "      <td>4217</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>2432</td>\n",
       "      <td>608376229893447680</td>\n",
       "      <td>80</td>\n",
       "      <td>1698</td>\n",
       "      <td>102</td>\n",
       "      <td>228</td>\n",
       "      <td>150</td>\n",
       "      <td>2874</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>2433</td>\n",
       "      <td>608234446081421312</td>\n",
       "      <td>62</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>251</td>\n",
       "      <td>215</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>2434</td>\n",
       "      <td>609831613887827968</td>\n",
       "      <td>56</td>\n",
       "      <td>64</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2278</td>\n",
       "      <td>56</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>2435</td>\n",
       "      <td>609695864274223104</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>65</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>2935</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2436</td>\n",
       "      <td>609382031659790336</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2474</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>2437</td>\n",
       "      <td>608607809031278592</td>\n",
       "      <td>72</td>\n",
       "      <td>59</td>\n",
       "      <td>113</td>\n",
       "      <td>99</td>\n",
       "      <td>165</td>\n",
       "      <td>6832</td>\n",
       "      <td>93</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>2438</td>\n",
       "      <td>609178554681675776</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>2439</td>\n",
       "      <td>608254479088144384</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4980</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>2440</td>\n",
       "      <td>609040663225602049</td>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4216</td>\n",
       "      <td>90</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>2441</td>\n",
       "      <td>607932867050504192</td>\n",
       "      <td>78</td>\n",
       "      <td>49</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>190</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>2442</td>\n",
       "      <td>608063741779951616</td>\n",
       "      <td>74</td>\n",
       "      <td>64</td>\n",
       "      <td>110</td>\n",
       "      <td>115</td>\n",
       "      <td>453</td>\n",
       "      <td>4993</td>\n",
       "      <td>379</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>2443</td>\n",
       "      <td>609059063113084928</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>1135</td>\n",
       "      <td>100</td>\n",
       "      <td>1091</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>2444</td>\n",
       "      <td>608959948077301760</td>\n",
       "      <td>56</td>\n",
       "      <td>51</td>\n",
       "      <td>97</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>1911</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>2445</td>\n",
       "      <td>609701774979563522</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1129</td>\n",
       "      <td>33</td>\n",
       "      <td>121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>2446</td>\n",
       "      <td>609034778738724864</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>880</td>\n",
       "      <td>9919</td>\n",
       "      <td>817</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>2447</td>\n",
       "      <td>607959208194154496</td>\n",
       "      <td>101</td>\n",
       "      <td>86</td>\n",
       "      <td>116</td>\n",
       "      <td>218</td>\n",
       "      <td>338</td>\n",
       "      <td>7670</td>\n",
       "      <td>237</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>2448</td>\n",
       "      <td>608887646086008832</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>91</td>\n",
       "      <td>71</td>\n",
       "      <td>197</td>\n",
       "      <td>4700</td>\n",
       "      <td>137</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>2449</td>\n",
       "      <td>608502553949650944</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>112</td>\n",
       "      <td>82</td>\n",
       "      <td>60</td>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>2450</td>\n",
       "      <td>609329602910097408</td>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>121</td>\n",
       "      <td>141</td>\n",
       "      <td>732</td>\n",
       "      <td>10612</td>\n",
       "      <td>652</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>2451</td>\n",
       "      <td>609082704458604545</td>\n",
       "      <td>76</td>\n",
       "      <td>91</td>\n",
       "      <td>122</td>\n",
       "      <td>81</td>\n",
       "      <td>12</td>\n",
       "      <td>2381</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>2452</td>\n",
       "      <td>608040173805445120</td>\n",
       "      <td>84</td>\n",
       "      <td>1697</td>\n",
       "      <td>57</td>\n",
       "      <td>92</td>\n",
       "      <td>233</td>\n",
       "      <td>1523</td>\n",
       "      <td>149</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>2453</td>\n",
       "      <td>609389112848547840</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1835</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>2454</td>\n",
       "      <td>609056814819323905</td>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>134</td>\n",
       "      <td>65</td>\n",
       "      <td>1311</td>\n",
       "      <td>3903</td>\n",
       "      <td>1255</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>2455</td>\n",
       "      <td>610125815116865536</td>\n",
       "      <td>86</td>\n",
       "      <td>67</td>\n",
       "      <td>116</td>\n",
       "      <td>102</td>\n",
       "      <td>350</td>\n",
       "      <td>3706</td>\n",
       "      <td>264</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>2456</td>\n",
       "      <td>608338587495628801</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4654</td>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>2457</td>\n",
       "      <td>609684420082180096</td>\n",
       "      <td>67</td>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "      <td>121</td>\n",
       "      <td>68</td>\n",
       "      <td>3482</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>2458</td>\n",
       "      <td>608392385425338368</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3297</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows  502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                  id  featNumCharPostText  \\\n",
       "0              0  608310377143799810                   59   \n",
       "1              1  609297109095972864                   75   \n",
       "2              2  609504474621612032                   70   \n",
       "3              3  609748367049105409                   58   \n",
       "4              4  608688782821453825                   70   \n",
       "5              5  609551038983475201                   45   \n",
       "6              6  609447408955719681                   74   \n",
       "7              7  609027430624288769                   56   \n",
       "8              8  608229011572068352                   75   \n",
       "9              9  609046214554755073                   71   \n",
       "10            10  609738211183751168                   70   \n",
       "11            11  609762671554990080                   38   \n",
       "12            12  608980783924178945                   65   \n",
       "13            13  609998126737395712                   70   \n",
       "14            14  608926452596273152                   56   \n",
       "15            15  609300383358418945                   65   \n",
       "16            16  608875876474851328                  120   \n",
       "17            17  610173844561788928                   59   \n",
       "18            18  609095030926635011                   95   \n",
       "19            19  608576531456249856                   48   \n",
       "20            20  609631563597389824                   44   \n",
       "21            21  609372816568360960                   86   \n",
       "22            22  610103181578764291                   44   \n",
       "23            23  609425861561962496                   74   \n",
       "24            24  608658774769958912                   35   \n",
       "25            25  609396415681445888                   63   \n",
       "26            26  609337131698229249                   53   \n",
       "27            27  610152342521028609                   59   \n",
       "28            28  608315579913629696                   53   \n",
       "29            29  609417622485139456                   61   \n",
       "...          ...                 ...                  ...   \n",
       "2429        2429  608881572834619392                   72   \n",
       "2430        2430  610096771746930688                   64   \n",
       "2431        2431  608289752438177792                   87   \n",
       "2432        2432  608376229893447680                   80   \n",
       "2433        2433  608234446081421312                   62   \n",
       "2434        2434  609831613887827968                   56   \n",
       "2435        2435  609695864274223104                   29   \n",
       "2436        2436  609382031659790336                   93   \n",
       "2437        2437  608607809031278592                   72   \n",
       "2438        2438  609178554681675776                   44   \n",
       "2439        2439  608254479088144384                   94   \n",
       "2440        2440  609040663225602049                   90   \n",
       "2441        2441  607932867050504192                   78   \n",
       "2442        2442  608063741779951616                   74   \n",
       "2443        2443  609059063113084928                   44   \n",
       "2444        2444  608959948077301760                   56   \n",
       "2445        2445  609701774979563522                   57   \n",
       "2446        2446  609034778738724864                   63   \n",
       "2447        2447  607959208194154496                  101   \n",
       "2448        2448  608887646086008832                   60   \n",
       "2449        2449  608502553949650944                   61   \n",
       "2450        2450  609329602910097408                   80   \n",
       "2451        2451  609082704458604545                   76   \n",
       "2452        2452  608040173805445120                   84   \n",
       "2453        2453  609389112848547840                   56   \n",
       "2454        2454  609056814819323905                   56   \n",
       "2455        2455  610125815116865536                   86   \n",
       "2456        2456  608338587495628801                   62   \n",
       "2457        2457  609684420082180096                   67   \n",
       "2458        2458  608392385425338368                   54   \n",
       "\n",
       "      featNumCharTargetTitle  featNumCharTargetDescription  \\\n",
       "0                         87                           141   \n",
       "1                         51                           119   \n",
       "2                         51                           133   \n",
       "3                         58                            94   \n",
       "4                         26                            81   \n",
       "5                         65                             0   \n",
       "6                         73                           102   \n",
       "7                         56                           130   \n",
       "8                         56                           101   \n",
       "9                         71                           134   \n",
       "10                        70                            99   \n",
       "11                        69                           110   \n",
       "12                        65                            32   \n",
       "13                       129                           145   \n",
       "14                        56                            53   \n",
       "15                        50                            95   \n",
       "16                        60                            77   \n",
       "17                        59                           103   \n",
       "18                        70                            67   \n",
       "19                       112                           153   \n",
       "20                        44                            94   \n",
       "21                        50                           107   \n",
       "22                        58                            72   \n",
       "23                        69                            65   \n",
       "24                        52                            88   \n",
       "25                        48                            37   \n",
       "26                        30                            63   \n",
       "27                        57                           117   \n",
       "28                        64                             0   \n",
       "29                        61                           146   \n",
       "...                      ...                           ...   \n",
       "2429                      48                            98   \n",
       "2430                      64                            95   \n",
       "2431                      33                            85   \n",
       "2432                    1698                           102   \n",
       "2433                      78                             0   \n",
       "2434                      64                           115   \n",
       "2435                      29                            65   \n",
       "2436                      76                             0   \n",
       "2437                      59                           113   \n",
       "2438                       4                           120   \n",
       "2439                      35                           106   \n",
       "2440                      77                           225   \n",
       "2441                      49                            61   \n",
       "2442                      64                           110   \n",
       "2443                      52                           100   \n",
       "2444                      51                            97   \n",
       "2445                      24                           178   \n",
       "2446                      57                            38   \n",
       "2447                      86                           116   \n",
       "2448                      60                            91   \n",
       "2449                      61                           112   \n",
       "2450                      21                           121   \n",
       "2451                      91                           122   \n",
       "2452                    1697                            57   \n",
       "2453                      87                             0   \n",
       "2454                      86                           134   \n",
       "2455                      67                           116   \n",
       "2456                      53                           108   \n",
       "2457                      25                            33   \n",
       "2458                      49                            84   \n",
       "\n",
       "      featNumCharTargetKeywords  featNumCharTargetCaptions  \\\n",
       "0                            66                       2059   \n",
       "1                           120                       1875   \n",
       "2                             0                        337   \n",
       "3                           120                        846   \n",
       "4                            47                        149   \n",
       "5                             0                          0   \n",
       "6                            61                          0   \n",
       "7                             0                          0   \n",
       "8                           144                        871   \n",
       "9                             0                        137   \n",
       "10                           44                         14   \n",
       "11                           37                          6   \n",
       "12                          101                          0   \n",
       "13                           17                         80   \n",
       "14                           77                        115   \n",
       "15                          241                       1662   \n",
       "16                           18                        144   \n",
       "17                           48                         15   \n",
       "18                           18                         54   \n",
       "19                           80                       1688   \n",
       "20                           54                        198   \n",
       "21                            0                         35   \n",
       "22                           82                          0   \n",
       "23                           39                          0   \n",
       "24                            0                          0   \n",
       "25                            0                          0   \n",
       "26                           59                      87180   \n",
       "27                          224                        189   \n",
       "28                            0                          0   \n",
       "29                          163                        257   \n",
       "...                         ...                        ...   \n",
       "2429                          0                         12   \n",
       "2430                         32                         13   \n",
       "2431                         50                        162   \n",
       "2432                        228                        150   \n",
       "2433                          0                        277   \n",
       "2434                          0                          0   \n",
       "2435                         41                         43   \n",
       "2436                          0                          0   \n",
       "2437                         99                        165   \n",
       "2438                          0                          0   \n",
       "2439                          0                         17   \n",
       "2440                          0                          0   \n",
       "2441                          0                         49   \n",
       "2442                        115                        453   \n",
       "2443                         95                       1135   \n",
       "2444                         61                         25   \n",
       "2445                          0                         24   \n",
       "2446                         39                        880   \n",
       "2447                        218                        338   \n",
       "2448                         71                        197   \n",
       "2449                         82                         60   \n",
       "2450                        141                        732   \n",
       "2451                         81                         12   \n",
       "2452                         92                        233   \n",
       "2453                          0                          0   \n",
       "2454                         65                       1311   \n",
       "2455                        102                        350   \n",
       "2456                          0                          0   \n",
       "2457                        121                         68   \n",
       "2458                          0                          0   \n",
       "\n",
       "      featNumCharTargetParagraphs  featDiffCharPostText_TargetCaptions  \\\n",
       "0                            2861                                 2000   \n",
       "1                             309                                 1800   \n",
       "2                            2903                                  267   \n",
       "3                            8127                                  788   \n",
       "4                            4006                                   79   \n",
       "5                            2503                                   45   \n",
       "6                            4345                                   74   \n",
       "7                             554                                   56   \n",
       "8                             252                                  796   \n",
       "9                            3229                                   66   \n",
       "10                           1727                                   56   \n",
       "11                           1525                                   32   \n",
       "12                           4709                                   65   \n",
       "13                           3425                                   10   \n",
       "14                           2488                                   59   \n",
       "15                            283                                 1597   \n",
       "16                           3132                                   24   \n",
       "17                           3140                                   44   \n",
       "18                           1450                                   41   \n",
       "19                           1221                                 1640   \n",
       "20                           5478                                  154   \n",
       "21                           1129                                   51   \n",
       "22                            581                                   44   \n",
       "23                           4212                                   74   \n",
       "24                            382                                   35   \n",
       "25                            395                                   63   \n",
       "26                           2371                                87127   \n",
       "27                           3975                                  130   \n",
       "28                            873                                   53   \n",
       "29                           5773                                  196   \n",
       "...                           ...                                  ...   \n",
       "2429                         2700                                   60   \n",
       "2430                         2087                                   51   \n",
       "2431                         4217                                   75   \n",
       "2432                         2874                                   70   \n",
       "2433                          251                                  215   \n",
       "2434                         2278                                   56   \n",
       "2435                         2935                                   14   \n",
       "2436                         2474                                   93   \n",
       "2437                         6832                                   93   \n",
       "2438                            0                                   44   \n",
       "2439                         4980                                   77   \n",
       "2440                         4216                                   90   \n",
       "2441                          190                                   29   \n",
       "2442                         4993                                  379   \n",
       "2443                          100                                 1091   \n",
       "2444                         1911                                   31   \n",
       "2445                         1129                                   33   \n",
       "2446                         9919                                  817   \n",
       "2447                         7670                                  237   \n",
       "2448                         4700                                  137   \n",
       "2449                         1002                                    1   \n",
       "2450                        10612                                  652   \n",
       "2451                         2381                                   64   \n",
       "2452                         1523                                  149   \n",
       "2453                         1835                                   56   \n",
       "2454                         3903                                 1255   \n",
       "2455                         3706                                  264   \n",
       "2456                         4654                                   62   \n",
       "2457                         3482                                    1   \n",
       "2458                         3297                                   54   \n",
       "\n",
       "      featDiffCharPostText_TargetDescription  \\\n",
       "0                                         82   \n",
       "1                                         44   \n",
       "2                                         63   \n",
       "3                                         36   \n",
       "4                                         11   \n",
       "5                                         45   \n",
       "6                                         28   \n",
       "7                                         74   \n",
       "8                                         26   \n",
       "9                                         63   \n",
       "10                                        29   \n",
       "11                                        72   \n",
       "12                                        33   \n",
       "13                                        75   \n",
       "14                                         3   \n",
       "15                                        30   \n",
       "16                                        43   \n",
       "17                                        44   \n",
       "18                                        28   \n",
       "19                                       105   \n",
       "20                                        50   \n",
       "21                                        21   \n",
       "22                                        28   \n",
       "23                                         9   \n",
       "24                                        53   \n",
       "25                                        26   \n",
       "26                                        10   \n",
       "27                                        58   \n",
       "28                                        53   \n",
       "29                                        85   \n",
       "...                                      ...   \n",
       "2429                                      26   \n",
       "2430                                      31   \n",
       "2431                                       2   \n",
       "2432                                      22   \n",
       "2433                                      62   \n",
       "2434                                      59   \n",
       "2435                                      36   \n",
       "2436                                      93   \n",
       "2437                                      41   \n",
       "2438                                      76   \n",
       "2439                                      12   \n",
       "2440                                     135   \n",
       "2441                                      17   \n",
       "2442                                      36   \n",
       "2443                                      56   \n",
       "2444                                      41   \n",
       "2445                                     121   \n",
       "2446                                      25   \n",
       "2447                                      15   \n",
       "2448                                      31   \n",
       "2449                                      51   \n",
       "2450                                      41   \n",
       "2451                                      46   \n",
       "2452                                      27   \n",
       "2453                                      56   \n",
       "2454                                      78   \n",
       "2455                                      30   \n",
       "2456                                      46   \n",
       "2457                                      34   \n",
       "2458                                      30   \n",
       "\n",
       "                      ...                   featCountPOS_WRB_NNP_NN  \\\n",
       "0                     ...                                         0   \n",
       "1                     ...                                         0   \n",
       "2                     ...                                         0   \n",
       "3                     ...                                         0   \n",
       "4                     ...                                         0   \n",
       "5                     ...                                         0   \n",
       "6                     ...                                         0   \n",
       "7                     ...                                         0   \n",
       "8                     ...                                         0   \n",
       "9                     ...                                         0   \n",
       "10                    ...                                         0   \n",
       "11                    ...                                         0   \n",
       "12                    ...                                         0   \n",
       "13                    ...                                         0   \n",
       "14                    ...                                         0   \n",
       "15                    ...                                         0   \n",
       "16                    ...                                         0   \n",
       "17                    ...                                         0   \n",
       "18                    ...                                         0   \n",
       "19                    ...                                         0   \n",
       "20                    ...                                         0   \n",
       "21                    ...                                         0   \n",
       "22                    ...                                         0   \n",
       "23                    ...                                         0   \n",
       "24                    ...                                         0   \n",
       "25                    ...                                         0   \n",
       "26                    ...                                         0   \n",
       "27                    ...                                         0   \n",
       "28                    ...                                         0   \n",
       "29                    ...                                         0   \n",
       "...                   ...                                       ...   \n",
       "2429                  ...                                         0   \n",
       "2430                  ...                                         0   \n",
       "2431                  ...                                         0   \n",
       "2432                  ...                                         0   \n",
       "2433                  ...                                         0   \n",
       "2434                  ...                                         0   \n",
       "2435                  ...                                         0   \n",
       "2436                  ...                                         0   \n",
       "2437                  ...                                         0   \n",
       "2438                  ...                                         0   \n",
       "2439                  ...                                         0   \n",
       "2440                  ...                                         0   \n",
       "2441                  ...                                         0   \n",
       "2442                  ...                                         0   \n",
       "2443                  ...                                         0   \n",
       "2444                  ...                                         0   \n",
       "2445                  ...                                         0   \n",
       "2446                  ...                                         0   \n",
       "2447                  ...                                         0   \n",
       "2448                  ...                                         0   \n",
       "2449                  ...                                         0   \n",
       "2450                  ...                                         0   \n",
       "2451                  ...                                         0   \n",
       "2452                  ...                                         0   \n",
       "2453                  ...                                         0   \n",
       "2454                  ...                                         0   \n",
       "2455                  ...                                         0   \n",
       "2456                  ...                                         0   \n",
       "2457                  ...                                         0   \n",
       "2458                  ...                                         0   \n",
       "\n",
       "      featCountPOS_WRB_NNP_NNS  featCountPOS_WRB_NNP_VBZ  \\\n",
       "0                            0                         0   \n",
       "1                            0                         0   \n",
       "2                            0                         0   \n",
       "3                            0                         0   \n",
       "4                            0                         0   \n",
       "5                            0                         0   \n",
       "6                            0                         0   \n",
       "7                            0                         0   \n",
       "8                            0                         0   \n",
       "9                            0                         0   \n",
       "10                           0                         0   \n",
       "11                           0                         0   \n",
       "12                           0                         0   \n",
       "13                           0                         0   \n",
       "14                           0                         0   \n",
       "15                           0                         0   \n",
       "16                           0                         0   \n",
       "17                           0                         0   \n",
       "18                           0                         0   \n",
       "19                           0                         0   \n",
       "20                           0                         0   \n",
       "21                           0                         0   \n",
       "22                           0                         0   \n",
       "23                           0                         0   \n",
       "24                           0                         0   \n",
       "25                           0                         0   \n",
       "26                           0                         0   \n",
       "27                           0                         0   \n",
       "28                           0                         0   \n",
       "29                           0                         0   \n",
       "...                        ...                       ...   \n",
       "2429                         0                         0   \n",
       "2430                         0                         0   \n",
       "2431                         0                         0   \n",
       "2432                         0                         0   \n",
       "2433                         0                         0   \n",
       "2434                         0                         0   \n",
       "2435                         0                         0   \n",
       "2436                         0                         0   \n",
       "2437                         0                         0   \n",
       "2438                         0                         0   \n",
       "2439                         0                         0   \n",
       "2440                         0                         0   \n",
       "2441                         0                         0   \n",
       "2442                         0                         0   \n",
       "2443                         0                         0   \n",
       "2444                         0                         0   \n",
       "2445                         0                         0   \n",
       "2446                         0                         0   \n",
       "2447                         0                         0   \n",
       "2448                         0                         0   \n",
       "2449                         0                         0   \n",
       "2450                         0                         0   \n",
       "2451                         0                         0   \n",
       "2452                         0                         0   \n",
       "2453                         0                         0   \n",
       "2454                         0                         0   \n",
       "2455                         0                         0   \n",
       "2456                         0                         0   \n",
       "2457                         0                         0   \n",
       "2458                         0                         0   \n",
       "\n",
       "      featCountPOS_WRB_RB_NN  featCountPOS_WRB_VBZ_NNS  featIsNEPresent  \\\n",
       "0                          0                         0                1   \n",
       "1                          0                         0                1   \n",
       "2                          0                         0                1   \n",
       "3                          0                         0                1   \n",
       "4                          0                         0                1   \n",
       "5                          0                         0                1   \n",
       "6                          0                         0                1   \n",
       "7                          0                         0                1   \n",
       "8                          0                         0                1   \n",
       "9                          0                         0                1   \n",
       "10                         0                         0                1   \n",
       "11                         0                         0                1   \n",
       "12                         0                         0                1   \n",
       "13                         0                         0                1   \n",
       "14                         0                         0                1   \n",
       "15                         0                         0                1   \n",
       "16                         0                         0                1   \n",
       "17                         0                         0                1   \n",
       "18                         0                         0                1   \n",
       "19                         0                         0                1   \n",
       "20                         0                         0                0   \n",
       "21                         0                         0                1   \n",
       "22                         0                         0                1   \n",
       "23                         0                         0                1   \n",
       "24                         0                         0                0   \n",
       "25                         0                         0                1   \n",
       "26                         0                         0                1   \n",
       "27                         0                         0                1   \n",
       "28                         0                         0                0   \n",
       "29                         0                         0                1   \n",
       "...                      ...                       ...              ...   \n",
       "2429                       0                         0                1   \n",
       "2430                       0                         0                1   \n",
       "2431                       0                         0                1   \n",
       "2432                       0                         0                1   \n",
       "2433                       0                         0                1   \n",
       "2434                       0                         0                1   \n",
       "2435                       0                         0                1   \n",
       "2436                       0                         0                1   \n",
       "2437                       0                         0                1   \n",
       "2438                       0                         0                1   \n",
       "2439                       0                         0                1   \n",
       "2440                       0                         0                0   \n",
       "2441                       0                         0                0   \n",
       "2442                       0                         0                1   \n",
       "2443                       0                         0                0   \n",
       "2444                       0                         0                1   \n",
       "2445                       0                         0                1   \n",
       "2446                       0                         0                1   \n",
       "2447                       0                         0                1   \n",
       "2448                       0                         0                1   \n",
       "2449                       0                         0                1   \n",
       "2450                       0                         0                0   \n",
       "2451                       0                         0                1   \n",
       "2452                       0                         0                1   \n",
       "2453                       0                         0                0   \n",
       "2454                       0                         0                1   \n",
       "2455                       0                         0                1   \n",
       "2456                       0                         0                1   \n",
       "2457                       0                         0                1   \n",
       "2458                       0                         0                1   \n",
       "\n",
       "      featSentiment  featSimilarityPostTextTargetTitle  \\\n",
       "0              0.40                               0.96   \n",
       "1              0.64                               0.75   \n",
       "2              0.34                               0.99   \n",
       "3              0.36                               1.00   \n",
       "4             -0.70                               0.87   \n",
       "5              0.00                               0.94   \n",
       "6              0.00                               0.70   \n",
       "7             -0.30                               1.00   \n",
       "8              0.36                               0.91   \n",
       "9              0.00                               1.00   \n",
       "10            -0.56                               1.00   \n",
       "11            -0.65                               0.93   \n",
       "12             0.00                               1.00   \n",
       "13            -0.74                               0.93   \n",
       "14            -0.40                               1.00   \n",
       "15            -0.25                               0.78   \n",
       "16             0.60                               0.84   \n",
       "17            -0.05                               1.00   \n",
       "18             0.36                               0.91   \n",
       "19             0.15                               0.94   \n",
       "20            -0.78                               1.00   \n",
       "21             0.23                               0.94   \n",
       "22            -0.51                               0.96   \n",
       "23            -0.49                               0.93   \n",
       "24             0.00                               0.96   \n",
       "25            -0.34                               0.76   \n",
       "26            -0.05                               0.61   \n",
       "27             0.00                               0.92   \n",
       "28             0.54                               0.92   \n",
       "29            -0.49                               0.83   \n",
       "...             ...                                ...   \n",
       "2429          -0.03                               0.76   \n",
       "2430          -0.60                               1.00   \n",
       "2431           0.00                               0.87   \n",
       "2432          -0.60                               0.86   \n",
       "2433           0.20                               0.93   \n",
       "2434           0.08                               0.98   \n",
       "2435           0.00                               1.00   \n",
       "2436          -0.57                               0.95   \n",
       "2437          -0.65                               0.98   \n",
       "2438           0.34                               0.16   \n",
       "2439           0.00                               0.77   \n",
       "2440           0.61                               0.77   \n",
       "2441           0.00                               0.50   \n",
       "2442          -0.27                               0.93   \n",
       "2443           0.00                               0.87   \n",
       "2444           0.36                               0.96   \n",
       "2445           0.00                               0.80   \n",
       "2446           0.00                               0.78   \n",
       "2447           0.00                               0.80   \n",
       "2448          -0.38                               1.00   \n",
       "2449           0.23                               1.00   \n",
       "2450          -0.15                               0.70   \n",
       "2451           0.00                               0.94   \n",
       "2452          -0.23                               0.82   \n",
       "2453           0.53                               0.94   \n",
       "2454           0.00                               0.93   \n",
       "2455           0.50                               0.75   \n",
       "2456           0.78                               0.95   \n",
       "2457           0.00                               0.32   \n",
       "2458          -0.38                               0.91   \n",
       "\n",
       "      featSimilarityPostTextTargetParagraphs  \\\n",
       "0                                       0.85   \n",
       "1                                       0.80   \n",
       "2                                       0.88   \n",
       "3                                       0.89   \n",
       "4                                       0.86   \n",
       "5                                       0.84   \n",
       "6                                       0.88   \n",
       "7                                       0.84   \n",
       "8                                       0.83   \n",
       "9                                       0.89   \n",
       "10                                      0.89   \n",
       "11                                      0.87   \n",
       "12                                      0.81   \n",
       "13                                      0.71   \n",
       "14                                      0.84   \n",
       "15                                      0.71   \n",
       "16                                      0.85   \n",
       "17                                      0.69   \n",
       "18                                      0.92   \n",
       "19                                      0.87   \n",
       "20                                      0.80   \n",
       "21                                      0.84   \n",
       "22                                      0.91   \n",
       "23                                      0.90   \n",
       "24                                      0.79   \n",
       "25                                      0.78   \n",
       "26                                      0.77   \n",
       "27                                      0.82   \n",
       "28                                      0.89   \n",
       "29                                      0.83   \n",
       "...                                      ...   \n",
       "2429                                    0.88   \n",
       "2430                                    0.87   \n",
       "2431                                    0.89   \n",
       "2432                                    0.90   \n",
       "2433                                    0.83   \n",
       "2434                                    0.83   \n",
       "2435                                    0.83   \n",
       "2436                                    0.92   \n",
       "2437                                    0.90   \n",
       "2438                                    0.00   \n",
       "2439                                    0.89   \n",
       "2440                                    0.85   \n",
       "2441                                    0.80   \n",
       "2442                                    0.87   \n",
       "2443                                    0.74   \n",
       "2444                                    0.83   \n",
       "2445                                    0.87   \n",
       "2446                                    0.66   \n",
       "2447                                    0.83   \n",
       "2448                                    0.83   \n",
       "2449                                    0.78   \n",
       "2450                                    0.87   \n",
       "2451                                    0.90   \n",
       "2452                                    0.79   \n",
       "2453                                    0.83   \n",
       "2454                                    0.88   \n",
       "2455                                    0.85   \n",
       "2456                                    0.89   \n",
       "2457                                    0.76   \n",
       "2458                                    0.70   \n",
       "\n",
       "      featSimilarityPostTextTargetKeywords  \n",
       "0                                     0.00  \n",
       "1                                     0.75  \n",
       "2                                     0.00  \n",
       "3                                     0.71  \n",
       "4                                     0.94  \n",
       "5                                     0.00  \n",
       "6                                     0.58  \n",
       "7                                     0.00  \n",
       "8                                     0.00  \n",
       "9                                     0.00  \n",
       "10                                    0.71  \n",
       "11                                    0.59  \n",
       "12                                    0.70  \n",
       "13                                    0.52  \n",
       "14                                    0.53  \n",
       "15                                    0.46  \n",
       "16                                    0.49  \n",
       "17                                    0.60  \n",
       "18                                    0.52  \n",
       "19                                    0.00  \n",
       "20                                    0.55  \n",
       "21                                    0.00  \n",
       "22                                    0.64  \n",
       "23                                    0.72  \n",
       "24                                    0.00  \n",
       "25                                    0.00  \n",
       "26                                    0.80  \n",
       "27                                    0.61  \n",
       "28                                    0.00  \n",
       "29                                    0.71  \n",
       "...                                    ...  \n",
       "2429                                  0.00  \n",
       "2430                                  0.59  \n",
       "2431                                  0.44  \n",
       "2432                                  0.79  \n",
       "2433                                  0.00  \n",
       "2434                                  0.00  \n",
       "2435                                  0.00  \n",
       "2436                                  0.00  \n",
       "2437                                  0.73  \n",
       "2438                                  0.00  \n",
       "2439                                  0.00  \n",
       "2440                                  0.00  \n",
       "2441                                  0.00  \n",
       "2442                                  0.56  \n",
       "2443                                  0.49  \n",
       "2444                                  0.00  \n",
       "2445                                  0.00  \n",
       "2446                                  0.50  \n",
       "2447                                  0.69  \n",
       "2448                                  0.72  \n",
       "2449                                  0.61  \n",
       "2450                                  0.43  \n",
       "2451                                  0.58  \n",
       "2452                                  0.83  \n",
       "2453                                  0.00  \n",
       "2454                                  0.00  \n",
       "2455                                  0.53  \n",
       "2456                                  0.00  \n",
       "2457                                  0.57  \n",
       "2458                                  0.00  \n",
       "\n",
       "[2459 rows x 502 columns]"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
