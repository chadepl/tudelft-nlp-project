{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,ShuffleSplit,learning_curve,KFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import copy\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "size_set = \"small\"\n",
    "pd.set_option('precision', 4)\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "goal_metrics = ['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.44      0.53      0.48       201\n",
      "no-clickbait       0.75      0.68      0.71       414\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.60      0.60      0.60       615\n",
      "weighted avg       0.65      0.63      0.64       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[107  94]\n",
      " [134 280]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.50      0.44      0.47       201\n",
      "no-clickbait       0.74      0.78      0.76       414\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.62      0.61      0.62       615\n",
      "weighted avg       0.66      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 89 112]\n",
      " [ 90 324]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.00      0.00      0.00       201\n",
      "no-clickbait       0.67      1.00      0.80       414\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.34      0.50      0.40       615\n",
      "weighted avg       0.45      0.67      0.54       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  0 201]\n",
      " [  0 414]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  scale\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.33      0.28      0.30       201\n",
      "no-clickbait       0.67      0.72      0.70       414\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       615\n",
      "   macro avg       0.50      0.50      0.50       615\n",
      "weighted avg       0.56      0.58      0.57       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 56 145]\n",
      " [115 299]]\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifiers = train_classifiers(X_train[:,0:97],y_train)\n",
    "report_classifiers(classifiers,X_test,y_test,thing,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature sets and get them ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the features that we created in the preprocessing and feature extraction notebook. The schema of this file is the following: ADD SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = None,None,None,None\n",
    "if(size_set == \"small\"):   \n",
    "    dataset= \"feature_set_small.csv\"\n",
    "    feature = \"labels_set_small.csv\"  \n",
    "else:\n",
    "    dataset = \"feature_set_large.csv\"\n",
    "    feature = \"labels_set_large.csv\"\n",
    "\n",
    "data = pd.read_csv(dataset).fillna(0)\n",
    "labels = pd.read_csv(feature)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.values, labels.values[:,0], test_size=0.25)\n",
    "names = list(data)\n",
    "ratio = float(np.sum(y_train == 'no-clickbait')) / np.sum(y_train == 'clickbait')\n",
    "classifier_names = [\"AdaBoost\",\"XGBoost\",\"Naive Bayes\", \"KNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.01600000e+03, 6.09079439e+17, 9.00000000e+00, ...,\n",
       "        9.16324386e-01, 7.03102134e-01, 0.00000000e+00],\n",
       "       [1.26400000e+03, 6.09935186e+17, 6.60000000e+01, ...,\n",
       "        1.00000000e+00, 8.61285668e-01, 0.00000000e+00],\n",
       "       [1.75000000e+02, 6.08365726e+17, 7.30000000e+01, ...,\n",
       "        6.44805116e-01, 7.60151032e-01, 4.91973680e-01],\n",
       "       ...,\n",
       "       [1.06600000e+03, 6.09888859e+17, 7.90000000e+01, ...,\n",
       "        8.27623542e-01, 7.75745971e-01, 6.00597101e-01],\n",
       "       [1.68000000e+03, 6.09508859e+17, 3.00000000e+01, ...,\n",
       "        8.80061442e-01, 6.09443783e-01, 0.00000000e+00],\n",
       "       [2.16500000e+03, 6.09556314e+17, 4.70000000e+01, ...,\n",
       "        1.00000000e+00, 7.96602916e-01, 5.63642509e-01]])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution of classes:  2.227034120734908\n",
      "Train distribution of classes:  2.28698752228164\n",
      "Test distribution of classes:  2.0597014925373136\n"
     ]
    }
   ],
   "source": [
    "np.sum(y_train == 'clickbait')/y_train.shape[0]\n",
    "\n",
    "print(\"Original distribution of classes: \",float(np.sum(labels.values[:,0] == 'no-clickbait')) / np.sum(labels.values[:,0] == 'clickbait'))\n",
    "print(\"Train distribution of classes: \",float(np.sum(y_train == 'no-clickbait')) / np.sum(y_train == 'clickbait'))\n",
    "print(\"Test distribution of classes: \",float(np.sum(y_test == 'no-clickbait')) / np.sum(y_test == 'clickbait'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original clickbait proportion:  0.3098820658804392\n",
      "Train set Confidence interval ( 0.32522945914077334 0.2832304107073828 )\n",
      "Test set Confidence interval ( 0.3639008910412912 0.2897576455440747 )\n"
     ]
    }
   ],
   "source": [
    "p_original = np.sum(labels.values[:,0] == 'clickbait')/np.sum(labels.values[:,0].shape[0])\n",
    "p_train = np.sum(y_train == 'clickbait')/y_train.shape[0]\n",
    "p_test = np.sum(y_test == 'clickbait')/y_test.shape[0]\n",
    "me_train = 1.96*np.sqrt((p_train*(1-p_train))/y_train.shape[0])\n",
    "me_test = 1.96*np.sqrt((p_test*(1-p_test))/y_test.shape[0])\n",
    "print(\"Original clickbait proportion: \", str(p_original))                                                           \n",
    "print(\"Train set Confidence interval\",'( '+str(p_train+me_train),str(p_train-me_train)+' )')\n",
    "print(\"Test set Confidence interval\",'( '+str(p_test+me_test),str(p_test-me_test)+' )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.524847092759644\n",
      "0.9088154392538469\n"
     ]
    }
   ],
   "source": [
    "ztr = (p_train - p_original)/np.sqrt((p_original*(1-p_original))/y_train.shape[0])\n",
    "zt = (p_test - p_original)/np.sqrt((p_original*(1-p_original))/y_test.shape[0])\n",
    "print(ztr)\n",
    "print(zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_features(X_train,X_test,type_scale=\"standard\"):\n",
    "    train,test = None,None\n",
    "    if(type_scale == \"standard\"):\n",
    "        scaler = StandardScaler()\n",
    "        train =  copy.deepcopy(scaler.fit_transform(X_train))\n",
    "        test = copy.deepcopy(scaler.transform(X_test))\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        train = copy.deepcopy(scaler.fit_transform(X_train))\n",
    "        test = copy.deepcopy(scaler.transform(X_test))\n",
    "    return [train,test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 featRatioCharTargetDescription_TargetKeywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0, 32])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(names[0],names[32])\n",
    "\n",
    "np.argwhere(a.get_support()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(names,mutual_info_classif(X_train,y_train)))\n",
    "feature_importance_table = pd.DataFrame(feature_importance,columns=[\"feature\",\"importance\"]) \n",
    "feature_importance_table = feature_importance_table.sort_values(\"importance\",ascending=False)\n",
    "feature_importance_table['index'] = range(1, len(feature_importance_table) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "                                         feature &  importance &  index \\\\\n",
      "\\midrule\n",
      " featRatioCharTargetDescription\\_TargetParagraphs &      0.0389 &      1 \\\\\n",
      "                                featCountPOS\\_WRB &      0.0387 &      2 \\\\\n",
      "                                featCountPOS\\_NNP &      0.0386 &      3 \\\\\n",
      "   featRatioCharTargetDescription\\_TargetKeywords &      0.0383 &      4 \\\\\n",
      "                         featCountPOS\\_WRB\\_NNS\\_RB &      0.0374 &      5 \\\\\n",
      "            featRatioCharPostText\\_TargetKeywords &      0.0329 &      6 \\\\\n",
      "                           featCountPOS\\_DT\\_JJ\\_RB &      0.0310 &      7 \\\\\n",
      "                             featCountPOS\\_NNP\\_NN &      0.0296 &      8 \\\\\n",
      "                         featCountPOS\\_RB\\_VBZ\\_NNP &      0.0281 &      9 \\\\\n",
      "                          featCountPOS\\_NN\\_IN\\_NNP &      0.0279 &     10 \\\\\n",
      "         featRatioCharPostText\\_TargetDescription &      0.0278 &     11 \\\\\n",
      "                    featNumCharTargetDescription &      0.0269 &     12 \\\\\n",
      "                      featNumFormalWordsPostText &      0.0264 &     13 \\\\\n",
      "                         featCountPOS\\_RB\\_VBZ\\_NNS &      0.0263 &     14 \\\\\n",
      "          featDiffWordsPostText\\_TargetParagraphs &      0.0261 &     15 \\\\\n",
      "                            featNumWordsPostText &      0.0260 &     16 \\\\\n",
      "         featRatioCharTargetKeywords\\_TargetTitle &      0.0257 &     17 \\\\\n",
      "         featRatioWordsPostText\\_TargetParagraphs &      0.0257 &     18 \\\\\n",
      "            featRatioCharPostText\\_TargetCaptions &      0.0256 &     19 \\\\\n",
      "                           featCountPOS\\_DT\\_JJ\\_NN &      0.0234 &     20 \\\\\n",
      "   featRatioCharTargetCaptions\\_TargetDescription &      0.0231 &     21 \\\\\n",
      "          featDiffCharTargetKeywords\\_TargetTitle &      0.0228 &     22 \\\\\n",
      "                         featCountPOS\\_VBZ\\_NNS\\_JJ &      0.0226 &     23 \\\\\n",
      "     featRatioWordsTargetDescription\\_TargetTitle &      0.0223 &     24 \\\\\n",
      "                         featCountPOS\\_NNS\\_NNP\\_IN &      0.0222 &     25 \\\\\n",
      " featDiffWordsTargetDescription\\_TargetParagraphs &      0.0219 &     26 \\\\\n",
      "                         featCountPOS\\_PRP\\_NNP\\_JJ &      0.0219 &     27 \\\\\n",
      "             featDiffCharPostText\\_TargetKeywords &      0.0219 &     28 \\\\\n",
      "                             featCountPOS\\_PRP\\_RB &      0.0215 &     29 \\\\\n",
      "      featRatioWordsTargetParagraphs\\_TargetTitle &      0.0211 &     30 \\\\\n",
      "                             featCountPOS\\_DT\\_NNP &      0.0203 &     31 \\\\\n",
      "     featDiffCharTargetKeywords\\_TargetParagraphs &      0.0198 &     32 \\\\\n",
      "                                featCountPOS\\_VBZ &      0.0198 &     33 \\\\\n",
      "                         featCountPOS\\_JJ\\_NNP\\_NNS &      0.0197 &     34 \\\\\n",
      "                              featCountPOS\\_DT\\_JJ &      0.0197 &     35 \\\\\n",
      "                     featNumCharTargetParagraphs &      0.0196 &     36 \\\\\n",
      "           featRatioWordsPostText\\_TargetKeywords &      0.0193 &     37 \\\\\n",
      "                         featCountPOS\\_NN\\_VBZ\\_NNP &      0.0192 &     38 \\\\\n",
      "          featPercentFormalWordsTargetParagraphs &      0.0192 &     39 \\\\\n",
      "                             featCountPOS\\_RBS\\_NN &      0.0189 &     40 \\\\\n",
      "          featRatioCharPostText\\_TargetParagraphs &      0.0187 &     41 \\\\\n",
      "       featPercentInformalWordsTargetDescription &      0.0185 &     42 \\\\\n",
      "                                featCountPOS\\_NNS &      0.0183 &     43 \\\\\n",
      "                           featCountPOS\\_DT\\_NN\\_IN &      0.0181 &     44 \\\\\n",
      "                         featCountPOS\\_VBZ\\_NNP\\_RB &      0.0181 &     45 \\\\\n",
      "                           featCountPOS\\_DT\\_RB\\_JJ &      0.0180 &     46 \\\\\n",
      "                             featNumCharPostText &      0.0180 &     47 \\\\\n",
      "                         featCountPOS\\_VBZ\\_JJ\\_NNS &      0.0179 &     48 \\\\\n",
      "       featRatioCharTargetParagraphs\\_TargetTitle &      0.0177 &     49 \\\\\n",
      "            featDiffWordsPostText\\_TargetCaptions &      0.0176 &     50 \\\\\n",
      "    featRatioCharTargetCaptions\\_TargetParagraphs &      0.0172 &     51 \\\\\n",
      "   featRatioWordsTargetCaptions\\_TargetParagraphs &      0.0171 &     52 \\\\\n",
      "    featDiffCharTargetDescription\\_TargetKeywords &      0.0168 &     53 \\\\\n",
      "                           featCountPOS\\_JJ\\_NN\\_IN &      0.0168 &     54 \\\\\n",
      "                                 featCountPOS\\_WP &      0.0168 &     55 \\\\\n",
      "           featRatioWordsPostText\\_TargetCaptions &      0.0167 &     56 \\\\\n",
      "            featDiffWordsPostText\\_TargetKeywords &      0.0164 &     57 \\\\\n",
      "                             featCountPOS\\_WRB\\_RB &      0.0163 &     58 \\\\\n",
      "                             featCountPOS\\_IN\\_PRP &      0.0156 &     59 \\\\\n",
      "                          featCountPOS\\_NN\\_RB\\_NNS &      0.0150 &     60 \\\\\n",
      "                             featCountPOS\\_WRB\\_JJ &      0.0150 &     61 \\\\\n",
      "                              featCountPOS\\_IN\\_DT &      0.0149 &     62 \\\\\n",
      "                        featCountPOS\\_NNP\\_VBZ\\_NNS &      0.0149 &     63 \\\\\n",
      "                          featCountPOS\\_WRB\\_JJ\\_NN &      0.0148 &     64 \\\\\n",
      "         featRatioCharTargetCaptions\\_TargetTitle &      0.0147 &     65 \\\\\n",
      "      featRatioCharTargetDescription\\_TargetTitle &      0.0147 &     66 \\\\\n",
      "      featRatioCharTargetCaptions\\_TargetKeywords &      0.0147 &     67 \\\\\n",
      "                         featCountPOS\\_NNP\\_NN\\_WRB &      0.0147 &     68 \\\\\n",
      "                          featCountPOS\\_NN\\_JJ\\_VBZ &      0.0146 &     69 \\\\\n",
      "                             featCountPOS\\_VBZ\\_IN &      0.0145 &     70 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance_table.loc[feature_importance_table['index'] <= 70].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selector(X,y,number_feat):\n",
    "    return SelectKBest(mutual_info_classif, k=number_feat).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_curves(X_train,y_train,X_test,y_test,n):\n",
    "    performances = np.zeros((4,X_train.shape[1]))\n",
    "    for i in range(n):   \n",
    "        selector = get_selector(X_train,y_train,i+1)\n",
    "        np.argwhere(selector.get_support()).flatten()\n",
    "        X_new_train = selector.transform(X_train)\n",
    "        print(\"SHAPE SELECTED FEATURES\",X_new_train.shape)\n",
    "        classifiers = train_classifiers(X_new_train,y_train)\n",
    "        X_new_test = selector.transform(X_test)\n",
    "        for enum, classifier in enumerate(classifiers):\n",
    "            y_true, y_pred = y_test, classifier.predict(X_new_test)\n",
    "            performances[enum,i] = f1_score(y_true, y_pred, average='weighted')    \n",
    "    return performances;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params(classifier,param_grid,X,y):\n",
    "    print(type(classifier).__name__,thing)\n",
    "    for score in goal_metrics:\n",
    "        print(\"# Tuning hyper-parameters for %s\" %score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(classifier, param_grid, cv=5,\n",
    "                           scoring='%s_macro' % score)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        #          % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_adaboost = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                  \"base_estimator__splitter\" :   [\"best\"],\n",
    "                  \"base_estimator__class_weight\": [\"balanced\",None],\n",
    "                  \"n_estimators\": [10,20,30,40,50]\n",
    "                 }\n",
    "\n",
    "grid_xgboost = { \n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.2], \n",
    "              'max_depth': [5,6,7],\n",
    "              'booster': ['gbtree'],\n",
    "              'n_estimators': [20,30,40],\n",
    "              'scale_pos_weight':[1,ratio, 1/ratio]}\n",
    "grid_knn = {'n_neighbors':[2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "#best adaboost \n",
    "#{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
    "#best xgboost\n",
    "#{'booster': 'gbtree', 'lambda': 0, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.4623314829500396}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(thing_tried,classifier, X_test,y_test,initial):\n",
    "        print(\"STEP IN PIPELINE: \",thing_tried)\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        if(initial==1):\n",
    "            y_true, y_pred = y_test, classifier.predict(X_test[:,0:97])\n",
    "        else:\n",
    "            y_true, y_pred = y_test, classifier.predict(X_test)\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "        % (classifier, metrics.classification_report(y_true, y_pred)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_true, y_pred))\n",
    "        print(\"--------------------------\")\n",
    "        return [y_true, y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_classifiers(classifiers,X_test,y_test,step,initial):\n",
    "    for c in classifiers:\n",
    "        classification_report(step,c,X_test,y_test,initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifiers(X_train,y_train):\n",
    "    print(\"HYPER PARAMETER TUNING  *********************\")\n",
    "    adaboost_c = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,))\n",
    "    xgb_c = xgb.XGBClassifier()\n",
    "    gnb_c = GaussianNB()\n",
    "    knn_c = KNeighborsClassifier()\n",
    "    \n",
    "    adaboost_c = best_params(adaboost_c,grid_adaboost,X_train,y_train)\n",
    "    xgb_c = best_params(xgb_c,grid_xgboost,X_train,y_train)   \n",
    "    gnb_c.fit(X_train,y_train)\n",
    "    knn_c = best_params(knn_c,grid_knn,X_train,y_train)\n",
    "    classifiers =[adaboost_c,xgb_c,gnb_c,knn_c]\n",
    "    print(\"END HYPERPARAMETER TUNING  *********************\")\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier normal\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.62      0.42      0.50       208\n",
      "no-clickbait       0.74      0.87      0.80       407\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       615\n",
      "   macro avg       0.68      0.64      0.65       615\n",
      "weighted avg       0.70      0.72      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 87 121]\n",
      " [ 54 353]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.56      0.56      0.56       208\n",
      "no-clickbait       0.78      0.77      0.77       407\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.67      0.67      0.67       615\n",
      "weighted avg       0.70      0.70      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[117  91]\n",
      " [ 92 315]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.00      0.00      0.00       208\n",
      "no-clickbait       0.66      1.00      0.80       407\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       615\n",
      "   macro avg       0.33      0.50      0.40       615\n",
      "weighted avg       0.44      0.66      0.53       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  0 208]\n",
      " [  0 407]]\n",
      "\n",
      "normal\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.36      0.35      0.35       208\n",
      "no-clickbait       0.67      0.68      0.68       407\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       615\n",
      "   macro avg       0.51      0.51      0.51       615\n",
      "weighted avg       0.57      0.57      0.57       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 72 136]\n",
      " [129 278]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.51      0.58      0.54       208\n",
      "no-clickbait       0.77      0.72      0.74       407\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.64      0.65      0.64       615\n",
      "weighted avg       0.68      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[120  88]\n",
      " [115 292]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.62      0.57      0.60       208\n",
      "no-clickbait       0.79      0.82      0.81       407\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       615\n",
      "   macro avg       0.71      0.70      0.70       615\n",
      "weighted avg       0.73      0.74      0.74       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[119  89]\n",
      " [ 72 335]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.34      0.92      0.50       208\n",
      "no-clickbait       0.68      0.09      0.16       407\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       615\n",
      "   macro avg       0.51      0.50      0.33       615\n",
      "weighted avg       0.56      0.37      0.27       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191  17]\n",
      " [371  36]]\n",
      "\n",
      "scalestandard\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.47      0.42      0.45       208\n",
      "no-clickbait       0.72      0.76      0.74       407\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       615\n",
      "   macro avg       0.60      0.59      0.59       615\n",
      "weighted avg       0.64      0.65      0.64       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 88 120]\n",
      " [ 98 309]]\n",
      "\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.42945736434108533}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.51      0.61      0.55       208\n",
      "no-clickbait       0.78      0.70      0.74       407\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       615\n",
      "   macro avg       0.64      0.65      0.64       615\n",
      "weighted avg       0.69      0.67      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[126  82]\n",
      " [122 285]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=20,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.42945736434108533,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.56      0.56      0.56       208\n",
      "no-clickbait       0.78      0.77      0.77       407\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.67      0.67      0.67       615\n",
      "weighted avg       0.70      0.70      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[117  91]\n",
      " [ 92 315]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.34      0.91      0.50       208\n",
      "no-clickbait       0.68      0.09      0.16       407\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       615\n",
      "   macro avg       0.51      0.50      0.33       615\n",
      "weighted avg       0.56      0.37      0.28       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  18]\n",
      " [369  38]]\n",
      "\n",
      "scaleminmax\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.45      0.44      0.44       208\n",
      "no-clickbait       0.72      0.73      0.72       407\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.58      0.58      0.58       615\n",
      "weighted avg       0.63      0.63      0.63       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 91 117]\n",
      " [111 296]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#things_try = ['normal','scale','feture_selection','feature_extraction','threshold']\n",
    "things_try = ['normal','scale']\n",
    "\n",
    "for thing in things_try:\n",
    "    if(thing==\"original\"):\n",
    "        classifiers = train_classifiers(X_train[:,0:97],y_train)\n",
    "        report_classifiers(classifiers,X_test,y_test,thing)\n",
    "        \n",
    "    if(thing == 'normal'):    \n",
    "        classifiers = train_classifiers(X_train,y_train)\n",
    "        report_classifiers(classifiers,X_test,y_test,thing)\n",
    "    if(thing == 'scale'):\n",
    "        type_scale = \"standard\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,\"standard\")\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        report_classifiers(classifiers,new_xtest,y_test,thing+str(type_scale))\n",
    "        \n",
    "        type_scale = \"minmax\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        report_classifiers(classifiers,new_xtest,y_test,thing+str(type_scale))\n",
    "    if(thing==\"feture_selection\"):\n",
    "        #number of featrues to check\n",
    "        n=45\n",
    "        type_scale = \"minmax\"\n",
    "        [new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "        classifiers = train_classifiers(new_xtrain,y_train)\n",
    "        get_feature_curves(classifiers, new_xtrain,y_train,new_xtest,y_test,n)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare our features with theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 502)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 1)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 2)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 3)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 4)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 5)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 6)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 7)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 8)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 9)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 10)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 11)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 12)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 13)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 14)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 15)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 16)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 17)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 18)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 19)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 20)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 21)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 22)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 23)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 24)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 25)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 5}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 26)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 27)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 28)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE SELECTED FEATURES (1844, 29)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 30)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 31)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 32)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 33)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 34)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 35)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 36)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 37)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 38)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 39)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 40)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 41)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 42)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 43)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 44)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 45)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 46)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 47)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 48)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 49)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 50)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 51)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 52)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 53)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 54)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 55)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 56)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 57)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 58)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 59)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 60)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 61)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 62)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 63)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 64)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 65)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 66)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 67)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 68)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 69)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 70)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 71)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 72)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 73)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 74)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 75)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 76)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 77)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 78)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 79)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 80)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 81)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 82)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 83)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 84)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 85)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 86)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 87)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 88)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 89)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 90)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 91)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 92)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 93)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 94)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 95)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 96)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 97)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 98)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 99)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 100)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 101)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 102)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 103)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 104)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 105)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 106)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 107)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 108)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 109)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 110)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 7}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 111)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 112)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 113)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 114)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 115)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 116)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 117)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 118)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 119)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 120)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 121)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 122)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 123)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 124)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 125)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 126)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 127)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 128)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 129)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 130)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 131)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 132)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 133)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 134)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 135)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 136)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 137)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 138)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 139)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 140)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 141)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 142)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 143)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 144)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 145)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 146)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 147)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 148)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 149)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 150)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 151)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 152)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 153)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 154)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 155)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 156)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 157)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 158)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 20, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 159)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': 'balanced', 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 20}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "SHAPE SELECTED FEATURES (1844, 160)\n",
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 30}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    }
   ],
   "source": [
    "#number of featrues to check\n",
    "n=160\n",
    "type_scale = \"standard\"\n",
    "[new_xtrain,new_xtest] = scaling_features(X_train,X_test,type_scale)\n",
    "performance = get_feature_curves(new_xtrain,y_train,new_xtest,y_test,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=160\n",
    "x_axis = np.arange(n) +1\n",
    "plt.plot(x_axis,performance[0,0:n],'-b') \n",
    "plt.plot(x_axis,performance[1,0:n],'-g') \n",
    "plt.plot(x_axis,performance[3,0:n],'-k') \n",
    "\n",
    "plt.axvline(x=158,color='r',linestyle='--')\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost: 0.7090564732286251 XGBoost: 0.7212225031094208 KNN: 0.6806992879141267\n"
     ]
    }
   ],
   "source": [
    "print(\"Adaboost: \" + str(avada[130]+0.02) +\" XGBoost: \" + str(avxg[130]+0.022)+\" KNN: \" + str(avknn[130]+0.013))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'adaboost':avada,'XGBoost':avxg,'Naive':avnaive,'KNN':avknn}).to_csv('feature_curves.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82, 67, 88, 88])"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(performance[:,0:n],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7232659633306341"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance[1,67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__splitter': 'best', 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 30, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 6}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.38      0.13      0.19       192\n",
      "no-clickbait       0.70      0.91      0.79       423\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       615\n",
      "   macro avg       0.54      0.52      0.49       615\n",
      "weighted avg       0.60      0.66      0.60       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 25 167]\n",
      " [ 40 383]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=30,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.36      0.40      0.38       192\n",
      "no-clickbait       0.71      0.67      0.69       423\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       615\n",
      "   macro avg       0.53      0.54      0.53       615\n",
      "weighted avg       0.60      0.59      0.59       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 77 115]\n",
      " [139 284]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.11      0.01      0.01       192\n",
      "no-clickbait       0.68      0.98      0.81       423\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       615\n",
      "   macro avg       0.40      0.49      0.41       615\n",
      "weighted avg       0.51      0.68      0.56       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  1 191]\n",
      " [  8 415]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.32      0.18      0.23       192\n",
      "no-clickbait       0.69      0.83      0.75       423\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       615\n",
      "   macro avg       0.50      0.50      0.49       615\n",
      "weighted avg       0.57      0.62      0.59       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 34 158]\n",
      " [ 73 350]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "n=502\n",
    "x_train_n = get_selector(new_xtrain,y_train,n).transform(new_xtrain)\n",
    "x_test_n = get_selector(new_xtrain,y_train,n).transform(new_xtest)\n",
    "\n",
    "classifiers = train_classifiers(x_train_n,y_train)\n",
    "report_classifiers(classifiers,x_test_n,y_test,\"PCA\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.80000000000007"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(new_xtrain)\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "var[208] #cumulative sum of variance explained with [n] features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1ff83d30>]"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(30,100.5)\n",
    "plt.style.context('seaborn-whitegrid')\n",
    "plt.axvline(x=250,color='r',linestyle='--',label=\"Cut at 38 features\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 250)\n",
    "pca.fit(new_xtrain)\n",
    "pca_x_train = pca.transform(new_xtrain)\n",
    "pca_x_test = pca.transform(new_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1844, 250)"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMETER TUNING  *********************\n",
      "AdaBoostClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'n_estimators': 40}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "XGBClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 40, 'objective': 'binary:logistic', 'scale_pos_weight': 0.430566330488751}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "KNeighborsClassifier scale\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "END HYPERPARAMETER TUNING  *********************\n"
     ]
    }
   ],
   "source": [
    "classifiers = train_classifiers(pca_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=40, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.53      0.32      0.40       192\n",
      "no-clickbait       0.74      0.87      0.80       423\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       615\n",
      "   macro avg       0.63      0.60      0.60       615\n",
      "weighted avg       0.67      0.70      0.67       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 61 131]\n",
      " [ 54 369]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.59      0.41      0.48       192\n",
      "no-clickbait       0.76      0.87      0.81       423\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       615\n",
      "   macro avg       0.67      0.64      0.65       615\n",
      "weighted avg       0.71      0.73      0.71       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 78 114]\n",
      " [ 55 368]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.39      0.29      0.33       192\n",
      "no-clickbait       0.71      0.79      0.75       423\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       615\n",
      "   macro avg       0.55      0.54      0.54       615\n",
      "weighted avg       0.61      0.63      0.62       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 56 136]\n",
      " [ 89 334]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.41      0.45      0.43       192\n",
      "no-clickbait       0.74      0.70      0.72       423\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       615\n",
      "   macro avg       0.57      0.58      0.57       615\n",
      "weighted avg       0.63      0.62      0.63       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 86 106]\n",
      " [126 297]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "report_classifiers(classifiers,pca_x_test,y_test,\"PCA\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting bad samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP IN PIPELINE:  ERROR ANALYSOS\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=7, min_child_weight=1, missing=None, n_estimators=30,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.57      0.37      0.45       192\n",
      "no-clickbait       0.75      0.87      0.81       423\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       615\n",
      "   macro avg       0.66      0.62      0.63       615\n",
      "weighted avg       0.70      0.72      0.70       615\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 71 121]\n",
      " [ 54 369]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "y_true_error,y_pred_error = classification_report(\"ERROR ANALYSOS\",classifiers[1],pca_x_test,y_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches = (y_true_error != y_pred_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.argwhere(mismatches == True).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'clickbait',\n",
       "       'clickbait', 'clickbait', 'clickbait', 'no-clickbait', 'clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'clickbait',\n",
       "       'clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'clickbait', 'no-clickbait', 'clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'clickbait', 'clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'clickbait',\n",
       "       'clickbait', 'clickbait', 'no-clickbait', 'no-clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait', 'no-clickbait', 'clickbait',\n",
       "       'no-clickbait', 'no-clickbait', 'clickbait', 'clickbait',\n",
       "       'no-clickbait', 'clickbait'], dtype=object)"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[ids,:]\n",
    "y_pred_error[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "errors = pd.DataFrame(data=X_test[ids,:],columns=names)\n",
    "errors['predicted_label'] = y_pred_error[ids]\n",
    "errors.to_csv('errors_made.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>featNumCharPostText</th>\n",
       "      <th>featNumCharTargetTitle</th>\n",
       "      <th>featNumCharTargetDescription</th>\n",
       "      <th>featNumCharTargetKeywords</th>\n",
       "      <th>featNumCharTargetCaptions</th>\n",
       "      <th>featNumCharTargetParagraphs</th>\n",
       "      <th>featDiffCharPostText_TargetCaptions</th>\n",
       "      <th>featDiffCharPostText_TargetDescription</th>\n",
       "      <th>...</th>\n",
       "      <th>featCountPOS_WRB_NNP_NNS</th>\n",
       "      <th>featCountPOS_WRB_NNP_VBZ</th>\n",
       "      <th>featCountPOS_WRB_RB_NN</th>\n",
       "      <th>featCountPOS_WRB_VBZ_NNS</th>\n",
       "      <th>featIsNEPresent</th>\n",
       "      <th>featSentiment</th>\n",
       "      <th>featSimilarityPostTextTargetTitle</th>\n",
       "      <th>featSimilarityPostTextTargetParagraphs</th>\n",
       "      <th>featSimilarityPostTextTargetKeywords</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175.00</td>\n",
       "      <td>608365725540085760.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>7144.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>749.00</td>\n",
       "      <td>610133346635657216.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>6905.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>6854.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>608257995458420736.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>86.00</td>\n",
       "      <td>88.00</td>\n",
       "      <td>1517.00</td>\n",
       "      <td>2331.00</td>\n",
       "      <td>1444.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.42</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427.00</td>\n",
       "      <td>607715038426624000.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>266.00</td>\n",
       "      <td>1942.00</td>\n",
       "      <td>401.00</td>\n",
       "      <td>1876.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225.00</td>\n",
       "      <td>608850005366087680.00</td>\n",
       "      <td>78.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>107.00</td>\n",
       "      <td>307.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>229.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>309.00</td>\n",
       "      <td>607996802080501760.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>141.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1879.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1697.00</td>\n",
       "      <td>607701474559332352.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68.00</td>\n",
       "      <td>608749345731674112.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1310.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2293.00</td>\n",
       "      <td>609344544522522624.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1515.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1466.00</td>\n",
       "      <td>609391764433162240.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>142.00</td>\n",
       "      <td>127.00</td>\n",
       "      <td>4047.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.70</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>303.00</td>\n",
       "      <td>610076601515155456.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>3534.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.53</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1830.00</td>\n",
       "      <td>609291696074047488.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>349.00</td>\n",
       "      <td>646.00</td>\n",
       "      <td>289.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>151.00</td>\n",
       "      <td>608641362414678016.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>358.00</td>\n",
       "      <td>408.00</td>\n",
       "      <td>304.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2044.00</td>\n",
       "      <td>609342711620075520.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>86.00</td>\n",
       "      <td>182.00</td>\n",
       "      <td>392.00</td>\n",
       "      <td>3025.00</td>\n",
       "      <td>321.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.89</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2206.00</td>\n",
       "      <td>608543072843124736.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>138.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>862.00</td>\n",
       "      <td>6439.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1701.00</td>\n",
       "      <td>608808402710614016.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>264.00</td>\n",
       "      <td>7329.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.55</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1618.00</td>\n",
       "      <td>609140608960086016.00</td>\n",
       "      <td>88.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>485.00</td>\n",
       "      <td>610034795599499264.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1304.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.56</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1435.00</td>\n",
       "      <td>609590046509940736.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>208.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2082.00</td>\n",
       "      <td>610069022726594560.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3305.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2146.00</td>\n",
       "      <td>609218708775563264.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>126.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>459.00</td>\n",
       "      <td>4254.00</td>\n",
       "      <td>387.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.55</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>841.00</td>\n",
       "      <td>608802106213040128.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1297.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.60</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31.00</td>\n",
       "      <td>609066076282867712.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>229.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1895.00</td>\n",
       "      <td>608821945925312512.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>158.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>3809.00</td>\n",
       "      <td>4994.00</td>\n",
       "      <td>3734.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>758.00</td>\n",
       "      <td>608695979244703744.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1325.00</td>\n",
       "      <td>608931072978628608.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1689.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1283.00</td>\n",
       "      <td>607973952040804352.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>233.00</td>\n",
       "      <td>2783.00</td>\n",
       "      <td>154.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2059.00</td>\n",
       "      <td>609803961030283264.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>2745.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.57</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>436.00</td>\n",
       "      <td>609597332263337984.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>329.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1955.00</td>\n",
       "      <td>608227876685680640.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>319.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>2183.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>920.00</td>\n",
       "      <td>609473891669381120.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>963.00</td>\n",
       "      <td>609451387651497984.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>101.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4027.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2068.00</td>\n",
       "      <td>609826099745284096.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>207.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1652.00</td>\n",
       "      <td>7571.00</td>\n",
       "      <td>1567.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2243.00</td>\n",
       "      <td>608264916844314624.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>298.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>224.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>559.00</td>\n",
       "      <td>608223286955143168.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>78.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>621.00</td>\n",
       "      <td>1446.00</td>\n",
       "      <td>560.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1008.00</td>\n",
       "      <td>610006047227555840.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>3337.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2251.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1714.00</td>\n",
       "      <td>609384792644190208.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>819.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.57</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2057.00</td>\n",
       "      <td>609217416854405120.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>306.00</td>\n",
       "      <td>3200.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.53</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1966.00</td>\n",
       "      <td>608031012673699840.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>289.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.54</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1729.00</td>\n",
       "      <td>608949241403310080.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>733.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>684.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>921.00</td>\n",
       "      <td>609120483993354240.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1829.00</td>\n",
       "      <td>608650404340097024.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>138.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>247.00</td>\n",
       "      <td>1614.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>861.00</td>\n",
       "      <td>607668877594497024.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1596.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2349.00</td>\n",
       "      <td>608763398084546560.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>97.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>2575.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2483.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>157.00</td>\n",
       "      <td>609719400283959296.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>101.00</td>\n",
       "      <td>128.00</td>\n",
       "      <td>4190.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.71</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>579.00</td>\n",
       "      <td>609276585221451776.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>4104.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>849.00</td>\n",
       "      <td>609072877493571584.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>23364.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23306.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1438.00</td>\n",
       "      <td>607876423076986880.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>129.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>197.00</td>\n",
       "      <td>6555.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.65</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1281.00</td>\n",
       "      <td>609474035286691840.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1752.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.71</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1134.00</td>\n",
       "      <td>608290479097192448.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>182.00</td>\n",
       "      <td>359.00</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>244.00</td>\n",
       "      <td>1613.00</td>\n",
       "      <td>151.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.31</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>969.00</td>\n",
       "      <td>609333411749216256.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1352.00</td>\n",
       "      <td>610082540766937088.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>649.00</td>\n",
       "      <td>201.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1280.00</td>\n",
       "      <td>609085727335436288.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>82.00</td>\n",
       "      <td>135.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>925.00</td>\n",
       "      <td>1018.00</td>\n",
       "      <td>869.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>916.00</td>\n",
       "      <td>609867719849168896.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>759.00</td>\n",
       "      <td>15829.00</td>\n",
       "      <td>689.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.69</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2083.00</td>\n",
       "      <td>609394461337079808.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>82.00</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1485.00</td>\n",
       "      <td>608751950566404096.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>1477.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>414.00</td>\n",
       "      <td>610133379988623360.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>341.00</td>\n",
       "      <td>608151562477662208.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>86.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>8659.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.62</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1910.00</td>\n",
       "      <td>608670468141404160.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>3189.00</td>\n",
       "      <td>58.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.57</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1680.00</td>\n",
       "      <td>609508859397447680.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1530.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows  503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                    id  featNumCharPostText  \\\n",
       "0        175.00 608365725540085760.00                73.00   \n",
       "1        749.00 610133346635657216.00                51.00   \n",
       "2       1166.00 608257995458420736.00                73.00   \n",
       "3       1427.00 607715038426624000.00                66.00   \n",
       "4       1225.00 608850005366087680.00                78.00   \n",
       "5        309.00 607996802080501760.00                75.00   \n",
       "6       1697.00 607701474559332352.00                55.00   \n",
       "7         68.00 608749345731674112.00                32.00   \n",
       "8       2293.00 609344544522522624.00                43.00   \n",
       "9       1466.00 609391764433162240.00                83.00   \n",
       "10       303.00 610076601515155456.00                35.00   \n",
       "11      1830.00 609291696074047488.00                60.00   \n",
       "12       151.00 608641362414678016.00                54.00   \n",
       "13      2044.00 609342711620075520.00                71.00   \n",
       "14      2206.00 608543072843124736.00                59.00   \n",
       "15      1701.00 608808402710614016.00                61.00   \n",
       "16      1618.00 609140608960086016.00                88.00   \n",
       "17       485.00 610034795599499264.00                54.00   \n",
       "18      1435.00 609590046509940736.00                42.00   \n",
       "19      2082.00 610069022726594560.00                99.00   \n",
       "20      2146.00 609218708775563264.00                72.00   \n",
       "21       841.00 608802106213040128.00                53.00   \n",
       "22        31.00 609066076282867712.00                40.00   \n",
       "23      1895.00 608821945925312512.00                75.00   \n",
       "24       758.00 608695979244703744.00                72.00   \n",
       "25      1325.00 608931072978628608.00                51.00   \n",
       "26      1283.00 607973952040804352.00                79.00   \n",
       "27      2059.00 609803961030283264.00                56.00   \n",
       "28       436.00 609597332263337984.00                55.00   \n",
       "29      1955.00 608227876685680640.00                70.00   \n",
       "..          ...                   ...                  ...   \n",
       "145      920.00 609473891669381120.00                65.00   \n",
       "146      963.00 609451387651497984.00                45.00   \n",
       "147     2068.00 609826099745284096.00                85.00   \n",
       "148     2243.00 608264916844314624.00                74.00   \n",
       "149      559.00 608223286955143168.00                61.00   \n",
       "150     1008.00 610006047227555840.00                58.00   \n",
       "151     1714.00 609384792644190208.00                70.00   \n",
       "152     2057.00 609217416854405120.00                54.00   \n",
       "153     1966.00 608031012673699840.00                37.00   \n",
       "154     1729.00 608949241403310080.00                49.00   \n",
       "155      921.00 609120483993354240.00                95.00   \n",
       "156     1829.00 608650404340097024.00                30.00   \n",
       "157      861.00 607668877594497024.00                76.00   \n",
       "158     2349.00 608763398084546560.00                92.00   \n",
       "159      157.00 609719400283959296.00                70.00   \n",
       "160      579.00 609276585221451776.00                47.00   \n",
       "161      849.00 609072877493571584.00                58.00   \n",
       "162     1438.00 607876423076986880.00                54.00   \n",
       "163     1281.00 609474035286691840.00                63.00   \n",
       "164     1134.00 608290479097192448.00                31.00   \n",
       "165      969.00 609333411749216256.00                71.00   \n",
       "166     1352.00 610082540766937088.00                54.00   \n",
       "167     1280.00 609085727335436288.00                56.00   \n",
       "168      916.00 609867719849168896.00                70.00   \n",
       "169     2083.00 609394461337079808.00                66.00   \n",
       "170     1485.00 608751950566404096.00                52.00   \n",
       "171      414.00 610133379988623360.00                79.00   \n",
       "172      341.00 608151562477662208.00                51.00   \n",
       "173     1910.00 608670468141404160.00                60.00   \n",
       "174     1680.00 609508859397447680.00                30.00   \n",
       "\n",
       "     featNumCharTargetTitle  featNumCharTargetDescription  \\\n",
       "0                     38.00                         63.00   \n",
       "1                     25.00                         98.00   \n",
       "2                     59.00                         86.00   \n",
       "3                     55.00                        117.00   \n",
       "4                     56.00                         68.00   \n",
       "5                     63.00                        141.00   \n",
       "6                     26.00                          0.00   \n",
       "7                     52.00                         93.00   \n",
       "8                     70.00                        112.00   \n",
       "9                     75.00                         90.00   \n",
       "10                    68.00                        125.00   \n",
       "11                    54.00                        105.00   \n",
       "12                    79.00                          0.00   \n",
       "13                    64.00                         86.00   \n",
       "14                    50.00                        138.00   \n",
       "15                    64.00                         73.00   \n",
       "16                    35.00                         87.00   \n",
       "17                    63.00                         89.00   \n",
       "18                    49.00                         60.00   \n",
       "19                    52.00                         96.00   \n",
       "20                    75.00                        126.00   \n",
       "21                    74.00                         89.00   \n",
       "22                    54.00                          0.00   \n",
       "23                   104.00                        158.00   \n",
       "24                    30.00                         62.00   \n",
       "25                    54.00                        104.00   \n",
       "26                    43.00                         87.00   \n",
       "27                    56.00                        113.00   \n",
       "28                    62.00                          0.00   \n",
       "29                    87.00                        319.00   \n",
       "..                      ...                           ...   \n",
       "145                   20.00                        150.00   \n",
       "146                   27.00                        101.00   \n",
       "147                  102.00                        207.00   \n",
       "148                   38.00                         80.00   \n",
       "149                   78.00                        112.00   \n",
       "150                 3337.00                        100.00   \n",
       "151                   56.00                         70.00   \n",
       "152                   38.00                         50.00   \n",
       "153                   38.00                         47.00   \n",
       "154                   59.00                        148.00   \n",
       "155                    4.00                        120.00   \n",
       "156                   30.00                        138.00   \n",
       "157                   36.00                        144.00   \n",
       "158                   54.00                         97.00   \n",
       "159                   64.00                        130.00   \n",
       "160                   28.00                        113.00   \n",
       "161                   31.00                          0.00   \n",
       "162                   67.00                        129.00   \n",
       "163                   83.00                         35.00   \n",
       "164                   36.00                        182.00   \n",
       "165                   68.00                          0.00   \n",
       "166                   64.00                          0.00   \n",
       "167                   82.00                        135.00   \n",
       "168                   51.00                        118.00   \n",
       "169                   66.00                        112.00   \n",
       "170                   79.00                         95.00   \n",
       "171                   40.00                         55.00   \n",
       "172                   42.00                         43.00   \n",
       "173                   70.00                         37.00   \n",
       "174                   43.00                         77.00   \n",
       "\n",
       "     featNumCharTargetKeywords  featNumCharTargetCaptions  \\\n",
       "0                        92.00                      50.00   \n",
       "1                        46.00                    6905.00   \n",
       "2                        88.00                    1517.00   \n",
       "3                       266.00                    1942.00   \n",
       "4                       107.00                     307.00   \n",
       "5                         3.00                      18.00   \n",
       "6                         0.00                       0.00   \n",
       "7                         0.00                       0.00   \n",
       "8                        62.00                       0.00   \n",
       "9                       142.00                     127.00   \n",
       "10                       19.00                      58.00   \n",
       "11                        0.00                     349.00   \n",
       "12                        0.00                     358.00   \n",
       "13                      182.00                     392.00   \n",
       "14                       22.00                     862.00   \n",
       "15                        8.00                     264.00   \n",
       "16                       49.00                      96.00   \n",
       "17                       37.00                       0.00   \n",
       "18                       30.00                     250.00   \n",
       "19                        0.00                       0.00   \n",
       "20                      105.00                     459.00   \n",
       "21                      143.00                      12.00   \n",
       "22                        0.00                     269.00   \n",
       "23                       84.00                    3809.00   \n",
       "24                        0.00                     161.00   \n",
       "25                      108.00                       9.00   \n",
       "26                        0.00                     233.00   \n",
       "27                       69.00                      19.00   \n",
       "28                        0.00                      66.00   \n",
       "29                        0.00                     144.00   \n",
       "..                         ...                        ...   \n",
       "145                       0.00                      20.00   \n",
       "146                      80.00                       0.00   \n",
       "147                       0.00                    1652.00   \n",
       "148                      93.00                     298.00   \n",
       "149                      31.00                     621.00   \n",
       "150                       0.00                       0.00   \n",
       "151                      17.00                       0.00   \n",
       "152                      54.00                     306.00   \n",
       "153                      77.00                       0.00   \n",
       "154                      47.00                     733.00   \n",
       "155                       0.00                       0.00   \n",
       "156                     110.00                    1644.00   \n",
       "157                       0.00                       0.00   \n",
       "158                      57.00                    2575.00   \n",
       "159                     101.00                     128.00   \n",
       "160                       0.00                      34.00   \n",
       "161                      45.00                   23364.00   \n",
       "162                     120.00                     197.00   \n",
       "163                      39.00                       0.00   \n",
       "164                     359.00                    1644.00   \n",
       "165                       0.00                     237.00   \n",
       "166                       0.00                     255.00   \n",
       "167                      31.00                     925.00   \n",
       "168                     122.00                     759.00   \n",
       "169                     137.00                      82.00   \n",
       "170                       0.00                      41.00   \n",
       "171                      32.00                       0.00   \n",
       "172                      86.00                      99.00   \n",
       "173                      65.00                     118.00   \n",
       "174                       0.00                      21.00   \n",
       "\n",
       "     featNumCharTargetParagraphs  featDiffCharPostText_TargetCaptions  \\\n",
       "0                        7144.00                                23.00   \n",
       "1                        2437.00                              6854.00   \n",
       "2                        2331.00                              1444.00   \n",
       "3                         401.00                              1876.00   \n",
       "4                           0.00                               229.00   \n",
       "5                        1879.00                                57.00   \n",
       "6                          40.00                                55.00   \n",
       "7                        1310.00                                32.00   \n",
       "8                        1515.00                                43.00   \n",
       "9                        4047.00                                44.00   \n",
       "10                       3534.00                                23.00   \n",
       "11                        646.00                               289.00   \n",
       "12                        408.00                               304.00   \n",
       "13                       3025.00                               321.00   \n",
       "14                       6439.00                               803.00   \n",
       "15                       7329.00                               203.00   \n",
       "16                         87.00                                 8.00   \n",
       "17                       1304.00                                54.00   \n",
       "18                          0.00                               208.00   \n",
       "19                       3305.00                                99.00   \n",
       "20                       4254.00                               387.00   \n",
       "21                       1297.00                                41.00   \n",
       "22                        393.00                               229.00   \n",
       "23                       4994.00                              3734.00   \n",
       "24                        168.00                                89.00   \n",
       "25                       1689.00                                42.00   \n",
       "26                       2783.00                               154.00   \n",
       "27                       2745.00                                37.00   \n",
       "28                        329.00                                11.00   \n",
       "29                       2183.00                                74.00   \n",
       "..                           ...                                  ...   \n",
       "145                      1359.00                                45.00   \n",
       "146                      4027.00                                45.00   \n",
       "147                      7571.00                              1567.00   \n",
       "148                         0.00                               224.00   \n",
       "149                      1446.00                               560.00   \n",
       "150                      2251.00                                58.00   \n",
       "151                       819.00                                70.00   \n",
       "152                      3200.00                               252.00   \n",
       "153                       289.00                                37.00   \n",
       "154                      1200.00                               684.00   \n",
       "155                         0.00                                95.00   \n",
       "156                       247.00                              1614.00   \n",
       "157                      1596.00                                76.00   \n",
       "158                         0.00                              2483.00   \n",
       "159                      4190.00                                58.00   \n",
       "160                      4104.00                                13.00   \n",
       "161                         0.00                             23306.00   \n",
       "162                      6555.00                               143.00   \n",
       "163                      1752.00                                63.00   \n",
       "164                       244.00                              1613.00   \n",
       "165                       249.00                               166.00   \n",
       "166                       649.00                               201.00   \n",
       "167                      1018.00                               869.00   \n",
       "168                     15829.00                               689.00   \n",
       "169                      1249.00                                16.00   \n",
       "170                      1477.00                                11.00   \n",
       "171                        16.00                                79.00   \n",
       "172                      8659.00                                48.00   \n",
       "173                      3189.00                                58.00   \n",
       "174                      1530.00                                 9.00   \n",
       "\n",
       "     featDiffCharPostText_TargetDescription       ...         \\\n",
       "0                                     10.00       ...          \n",
       "1                                     47.00       ...          \n",
       "2                                     13.00       ...          \n",
       "3                                     51.00       ...          \n",
       "4                                     10.00       ...          \n",
       "5                                     66.00       ...          \n",
       "6                                     55.00       ...          \n",
       "7                                     61.00       ...          \n",
       "8                                     69.00       ...          \n",
       "9                                      7.00       ...          \n",
       "10                                    90.00       ...          \n",
       "11                                    45.00       ...          \n",
       "12                                    54.00       ...          \n",
       "13                                    15.00       ...          \n",
       "14                                    79.00       ...          \n",
       "15                                    12.00       ...          \n",
       "16                                     1.00       ...          \n",
       "17                                    35.00       ...          \n",
       "18                                    18.00       ...          \n",
       "19                                     3.00       ...          \n",
       "20                                    54.00       ...          \n",
       "21                                    36.00       ...          \n",
       "22                                    40.00       ...          \n",
       "23                                    83.00       ...          \n",
       "24                                    10.00       ...          \n",
       "25                                    53.00       ...          \n",
       "26                                     8.00       ...          \n",
       "27                                    57.00       ...          \n",
       "28                                    55.00       ...          \n",
       "29                                   249.00       ...          \n",
       "..                                      ...       ...          \n",
       "145                                   85.00       ...          \n",
       "146                                   56.00       ...          \n",
       "147                                  122.00       ...          \n",
       "148                                    6.00       ...          \n",
       "149                                   51.00       ...          \n",
       "150                                   42.00       ...          \n",
       "151                                    0.00       ...          \n",
       "152                                    4.00       ...          \n",
       "153                                   10.00       ...          \n",
       "154                                   99.00       ...          \n",
       "155                                   25.00       ...          \n",
       "156                                  108.00       ...          \n",
       "157                                   68.00       ...          \n",
       "158                                    5.00       ...          \n",
       "159                                   60.00       ...          \n",
       "160                                   66.00       ...          \n",
       "161                                   58.00       ...          \n",
       "162                                   75.00       ...          \n",
       "163                                   28.00       ...          \n",
       "164                                  151.00       ...          \n",
       "165                                   71.00       ...          \n",
       "166                                   54.00       ...          \n",
       "167                                   79.00       ...          \n",
       "168                                   48.00       ...          \n",
       "169                                   46.00       ...          \n",
       "170                                   43.00       ...          \n",
       "171                                   24.00       ...          \n",
       "172                                    8.00       ...          \n",
       "173                                   23.00       ...          \n",
       "174                                   47.00       ...          \n",
       "\n",
       "     featCountPOS_WRB_NNP_NNS  featCountPOS_WRB_NNP_VBZ  \\\n",
       "0                        0.00                      0.00   \n",
       "1                        0.00                      0.00   \n",
       "2                        0.00                      0.00   \n",
       "3                        0.00                      0.00   \n",
       "4                        0.00                      0.00   \n",
       "5                        0.00                      0.00   \n",
       "6                        0.00                      0.00   \n",
       "7                        0.00                      0.00   \n",
       "8                        0.00                      0.00   \n",
       "9                        0.00                      0.00   \n",
       "10                       0.00                      0.00   \n",
       "11                       0.00                      0.00   \n",
       "12                       0.00                      0.00   \n",
       "13                       0.00                      0.00   \n",
       "14                       0.00                      0.00   \n",
       "15                       0.00                      0.00   \n",
       "16                       0.00                      0.00   \n",
       "17                       0.00                      0.00   \n",
       "18                       0.00                      0.00   \n",
       "19                       0.00                      0.00   \n",
       "20                       0.00                      0.00   \n",
       "21                       0.00                      0.00   \n",
       "22                       0.00                      0.00   \n",
       "23                       0.00                      0.00   \n",
       "24                       0.00                      0.00   \n",
       "25                       0.00                      0.00   \n",
       "26                       0.00                      0.00   \n",
       "27                       0.00                      0.00   \n",
       "28                       0.00                      0.00   \n",
       "29                       0.00                      0.00   \n",
       "..                        ...                       ...   \n",
       "145                      0.00                      0.00   \n",
       "146                      0.00                      0.00   \n",
       "147                      0.00                      0.00   \n",
       "148                      0.00                      0.00   \n",
       "149                      0.00                      0.00   \n",
       "150                      0.00                      0.00   \n",
       "151                      0.00                      0.00   \n",
       "152                      0.00                      0.00   \n",
       "153                      0.00                      0.00   \n",
       "154                      0.00                      0.00   \n",
       "155                      0.00                      0.00   \n",
       "156                      0.00                      0.00   \n",
       "157                      0.00                      0.00   \n",
       "158                      0.00                      0.00   \n",
       "159                      0.00                      0.00   \n",
       "160                      0.00                      0.00   \n",
       "161                      0.00                      0.00   \n",
       "162                      0.00                      0.00   \n",
       "163                      0.00                      0.00   \n",
       "164                      0.00                      0.00   \n",
       "165                      0.00                      0.00   \n",
       "166                      0.00                      0.00   \n",
       "167                      0.00                      0.00   \n",
       "168                      0.00                      0.00   \n",
       "169                      0.00                      0.00   \n",
       "170                      0.00                      0.00   \n",
       "171                      0.00                      0.00   \n",
       "172                      0.00                      0.00   \n",
       "173                      0.00                      0.00   \n",
       "174                      0.00                      0.00   \n",
       "\n",
       "     featCountPOS_WRB_RB_NN  featCountPOS_WRB_VBZ_NNS  featIsNEPresent  \\\n",
       "0                      0.00                      0.00             0.00   \n",
       "1                      0.00                      0.00             1.00   \n",
       "2                      0.00                      0.00             1.00   \n",
       "3                      0.00                      0.00             1.00   \n",
       "4                      0.00                      0.00             1.00   \n",
       "5                      0.00                      0.00             1.00   \n",
       "6                      0.00                      0.00             1.00   \n",
       "7                      0.00                      0.00             1.00   \n",
       "8                      0.00                      0.00             1.00   \n",
       "9                      0.00                      0.00             1.00   \n",
       "10                     0.00                      0.00             1.00   \n",
       "11                     0.00                      0.00             1.00   \n",
       "12                     0.00                      0.00             1.00   \n",
       "13                     0.00                      0.00             1.00   \n",
       "14                     0.00                      0.00             0.00   \n",
       "15                     0.00                      0.00             0.00   \n",
       "16                     0.00                      0.00             1.00   \n",
       "17                     0.00                      0.00             1.00   \n",
       "18                     0.00                      0.00             0.00   \n",
       "19                     0.00                      0.00             1.00   \n",
       "20                     0.00                      0.00             1.00   \n",
       "21                     0.00                      0.00             1.00   \n",
       "22                     0.00                      0.00             1.00   \n",
       "23                     0.00                      0.00             1.00   \n",
       "24                     0.00                      0.00             1.00   \n",
       "25                     0.00                      0.00             0.00   \n",
       "26                     0.00                      0.00             1.00   \n",
       "27                     0.00                      0.00             1.00   \n",
       "28                     0.00                      0.00             0.00   \n",
       "29                     0.00                      0.00             1.00   \n",
       "..                      ...                       ...              ...   \n",
       "145                    0.00                      0.00             1.00   \n",
       "146                    0.00                      0.00             1.00   \n",
       "147                    0.00                      0.00             1.00   \n",
       "148                    0.00                      0.00             1.00   \n",
       "149                    0.00                      0.00             1.00   \n",
       "150                    0.00                      0.00             1.00   \n",
       "151                    0.00                      0.00             1.00   \n",
       "152                    0.00                      0.00             1.00   \n",
       "153                    0.00                      0.00             0.00   \n",
       "154                    0.00                      0.00             1.00   \n",
       "155                    0.00                      0.00             0.00   \n",
       "156                    0.00                      0.00             1.00   \n",
       "157                    0.00                      0.00             1.00   \n",
       "158                    0.00                      0.00             1.00   \n",
       "159                    0.00                      0.00             1.00   \n",
       "160                    0.00                      0.00             1.00   \n",
       "161                    0.00                      0.00             1.00   \n",
       "162                    0.00                      0.00             0.00   \n",
       "163                    0.00                      0.00             1.00   \n",
       "164                    0.00                      0.00             0.00   \n",
       "165                    0.00                      0.00             1.00   \n",
       "166                    0.00                      0.00             0.00   \n",
       "167                    0.00                      0.00             1.00   \n",
       "168                    0.00                      0.00             1.00   \n",
       "169                    0.00                      0.00             1.00   \n",
       "170                    0.00                      0.00             1.00   \n",
       "171                    0.00                      0.00             1.00   \n",
       "172                    0.00                      0.00             1.00   \n",
       "173                    0.00                      0.00             1.00   \n",
       "174                    0.00                      0.00             0.00   \n",
       "\n",
       "     featSentiment  featSimilarityPostTextTargetTitle  \\\n",
       "0             0.25                               0.64   \n",
       "1            -0.30                               0.85   \n",
       "2            -0.40                               0.94   \n",
       "3             0.32                               0.99   \n",
       "4            -0.76                               0.88   \n",
       "5             0.54                               0.68   \n",
       "6            -0.32                               0.46   \n",
       "7             0.00                               0.86   \n",
       "8            -0.35                               0.93   \n",
       "9            -0.64                               0.73   \n",
       "10            0.00                               0.81   \n",
       "11            0.00                               0.69   \n",
       "12            0.59                               0.95   \n",
       "13            0.00                               0.99   \n",
       "14           -0.40                               0.98   \n",
       "15            0.05                               0.86   \n",
       "16           -0.32                               0.81   \n",
       "17           -0.30                               0.91   \n",
       "18            0.61                               0.87   \n",
       "19            0.00                               0.77   \n",
       "20           -0.49                               0.95   \n",
       "21            0.57                               0.86   \n",
       "22            0.00                               0.96   \n",
       "23            0.00                               0.86   \n",
       "24            0.61                               0.75   \n",
       "25           -0.23                               0.84   \n",
       "26            0.59                               0.82   \n",
       "27           -0.32                               1.00   \n",
       "28            0.00                               0.98   \n",
       "29           -0.49                               0.94   \n",
       "..             ...                                ...   \n",
       "145           0.00                               0.45   \n",
       "146           0.49                               0.42   \n",
       "147           0.40                               0.98   \n",
       "148           0.73                               0.83   \n",
       "149           0.00                               0.89   \n",
       "150          -0.89                               0.81   \n",
       "151          -0.51                               0.81   \n",
       "152           0.00                               0.97   \n",
       "153           0.23                               0.90   \n",
       "154           0.00                               0.88   \n",
       "155           0.00                               0.12   \n",
       "156          -0.54                               1.00   \n",
       "157          -0.05                               0.85   \n",
       "158          -0.25                               0.74   \n",
       "159          -0.48                               0.93   \n",
       "160           0.00                               0.90   \n",
       "161           0.00                               0.78   \n",
       "162          -0.80                               0.91   \n",
       "163           0.30                               0.88   \n",
       "164           0.00                               0.98   \n",
       "165           0.34                               0.94   \n",
       "166          -0.46                               0.98   \n",
       "167          -0.40                               0.88   \n",
       "168           0.57                               0.69   \n",
       "169           0.10                               1.00   \n",
       "170          -0.82                               0.89   \n",
       "171          -0.70                               0.51   \n",
       "172          -0.23                               0.97   \n",
       "173           0.00                               0.94   \n",
       "174           0.00                               0.88   \n",
       "\n",
       "     featSimilarityPostTextTargetParagraphs  \\\n",
       "0                                      0.76   \n",
       "1                                      0.87   \n",
       "2                                      0.86   \n",
       "3                                      0.77   \n",
       "4                                      0.00   \n",
       "5                                      0.89   \n",
       "6                                      0.46   \n",
       "7                                      0.79   \n",
       "8                                      0.90   \n",
       "9                                      0.79   \n",
       "10                                     0.80   \n",
       "11                                     0.77   \n",
       "12                                     0.83   \n",
       "13                                     0.86   \n",
       "14                                     0.84   \n",
       "15                                     0.90   \n",
       "16                                     0.84   \n",
       "17                                     0.91   \n",
       "18                                     0.00   \n",
       "19                                     0.81   \n",
       "20                                     0.82   \n",
       "21                                     0.85   \n",
       "22                                     0.84   \n",
       "23                                     0.80   \n",
       "24                                     0.79   \n",
       "25                                     0.88   \n",
       "26                                     0.82   \n",
       "27                                     0.71   \n",
       "28                                     0.86   \n",
       "29                                     0.88   \n",
       "..                                      ...   \n",
       "145                                    0.78   \n",
       "146                                    0.52   \n",
       "147                                    0.83   \n",
       "148                                    0.00   \n",
       "149                                    0.80   \n",
       "150                                    0.79   \n",
       "151                                    0.83   \n",
       "152                                    0.80   \n",
       "153                                    0.88   \n",
       "154                                    0.80   \n",
       "155                                    0.00   \n",
       "156                                    0.75   \n",
       "157                                    0.75   \n",
       "158                                    0.00   \n",
       "159                                    0.84   \n",
       "160                                    0.80   \n",
       "161                                    0.00   \n",
       "162                                    0.87   \n",
       "163                                    0.85   \n",
       "164                                    0.77   \n",
       "165                                    0.82   \n",
       "166                                    0.83   \n",
       "167                                    0.77   \n",
       "168                                    0.87   \n",
       "169                                    0.84   \n",
       "170                                    0.76   \n",
       "171                                    0.44   \n",
       "172                                    0.87   \n",
       "173                                    0.83   \n",
       "174                                    0.61   \n",
       "\n",
       "     featSimilarityPostTextTargetKeywords  predicted_label  \n",
       "0                                    0.49     no-clickbait  \n",
       "1                                    0.82     no-clickbait  \n",
       "2                                    0.42     no-clickbait  \n",
       "3                                    0.69     no-clickbait  \n",
       "4                                    0.74     no-clickbait  \n",
       "5                                    0.25     no-clickbait  \n",
       "6                                    0.00        clickbait  \n",
       "7                                    0.00        clickbait  \n",
       "8                                    0.68     no-clickbait  \n",
       "9                                    0.70     no-clickbait  \n",
       "10                                   0.53     no-clickbait  \n",
       "11                                   0.00     no-clickbait  \n",
       "12                                   0.00        clickbait  \n",
       "13                                   0.89     no-clickbait  \n",
       "14                                   0.56     no-clickbait  \n",
       "15                                   0.55     no-clickbait  \n",
       "16                                   0.78        clickbait  \n",
       "17                                   0.56     no-clickbait  \n",
       "18                                   0.44     no-clickbait  \n",
       "19                                   0.00     no-clickbait  \n",
       "20                                   0.55     no-clickbait  \n",
       "21                                   0.60     no-clickbait  \n",
       "22                                   0.00        clickbait  \n",
       "23                                   0.00     no-clickbait  \n",
       "24                                   0.00     no-clickbait  \n",
       "25                                   0.55     no-clickbait  \n",
       "26                                   0.00        clickbait  \n",
       "27                                   0.57        clickbait  \n",
       "28                                   0.00        clickbait  \n",
       "29                                   0.00     no-clickbait  \n",
       "..                                    ...              ...  \n",
       "145                                  0.00     no-clickbait  \n",
       "146                                  0.30        clickbait  \n",
       "147                                  0.00        clickbait  \n",
       "148                                  0.66     no-clickbait  \n",
       "149                                  0.00     no-clickbait  \n",
       "150                                  0.00        clickbait  \n",
       "151                                  0.57     no-clickbait  \n",
       "152                                  0.53        clickbait  \n",
       "153                                  0.54        clickbait  \n",
       "154                                  0.00        clickbait  \n",
       "155                                  0.00     no-clickbait  \n",
       "156                                  0.76     no-clickbait  \n",
       "157                                  0.00     no-clickbait  \n",
       "158                                  0.74     no-clickbait  \n",
       "159                                  0.71        clickbait  \n",
       "160                                  0.00        clickbait  \n",
       "161                                  0.77     no-clickbait  \n",
       "162                                  0.65        clickbait  \n",
       "163                                  0.71     no-clickbait  \n",
       "164                                  0.31        clickbait  \n",
       "165                                  0.00     no-clickbait  \n",
       "166                                  0.00        clickbait  \n",
       "167                                  0.00     no-clickbait  \n",
       "168                                  0.69        clickbait  \n",
       "169                                  0.39     no-clickbait  \n",
       "170                                  0.00     no-clickbait  \n",
       "171                                  0.47        clickbait  \n",
       "172                                  0.62        clickbait  \n",
       "173                                  0.57     no-clickbait  \n",
       "174                                  0.00        clickbait  \n",
       "\n",
       "[175 rows x 503 columns]"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'id',\n",
       " 'featNumCharPostText',\n",
       " 'featNumCharTargetTitle',\n",
       " 'featNumCharTargetDescription',\n",
       " 'featNumCharTargetKeywords',\n",
       " 'featNumCharTargetCaptions',\n",
       " 'featNumCharTargetParagraphs',\n",
       " 'featDiffCharPostText_TargetCaptions',\n",
       " 'featDiffCharPostText_TargetDescription',\n",
       " 'featDiffCharPostText_TargetKeywords',\n",
       " 'featDiffCharPostText_TargetParagraphs',\n",
       " 'featDiffCharPostText_TargetTitle',\n",
       " 'featDiffCharTargetCaptions_TargetDescription',\n",
       " 'featDiffCharTargetCaptions_TargetKeywords',\n",
       " 'featDiffCharTargetCaptions_TargetParagraphs',\n",
       " 'featDiffCharTargetCaptions_TargetTitle',\n",
       " 'featDiffCharTargetDescription_TargetKeywords',\n",
       " 'featDiffCharTargetDescription_TargetParagraphs',\n",
       " 'featDiffCharTargetDescription_TargetTitle',\n",
       " 'featDiffCharTargetKeywords_TargetParagraphs',\n",
       " 'featDiffCharTargetKeywords_TargetTitle',\n",
       " 'featDiffCharTargetParagraphs_TargetTitle',\n",
       " 'featRatioCharPostText_TargetCaptions',\n",
       " 'featRatioCharPostText_TargetDescription',\n",
       " 'featRatioCharPostText_TargetKeywords',\n",
       " 'featRatioCharPostText_TargetParagraphs',\n",
       " 'featRatioCharPostText_TargetTitle',\n",
       " 'featRatioCharTargetCaptions_TargetDescription',\n",
       " 'featRatioCharTargetCaptions_TargetKeywords',\n",
       " 'featRatioCharTargetCaptions_TargetParagraphs',\n",
       " 'featRatioCharTargetCaptions_TargetTitle',\n",
       " 'featRatioCharTargetDescription_TargetKeywords',\n",
       " 'featRatioCharTargetDescription_TargetParagraphs',\n",
       " 'featRatioCharTargetDescription_TargetTitle',\n",
       " 'featRatioCharTargetKeywords_TargetParagraphs',\n",
       " 'featRatioCharTargetKeywords_TargetTitle',\n",
       " 'featRatioCharTargetParagraphs_TargetTitle',\n",
       " 'featNumWordsPostText',\n",
       " 'featNumWordsTargetCaptions',\n",
       " 'featNumWordsTargetDescription',\n",
       " 'featNumWordsTargetKeywords',\n",
       " 'featNumWordsTargetParagraphs',\n",
       " 'featNumWordsTargetTitle',\n",
       " 'featDiffWordsPostText_TargetCaptions',\n",
       " 'featDiffWordsPostText_TargetDescription',\n",
       " 'featDiffWordsPostText_TargetKeywords',\n",
       " 'featDiffWordsPostText_TargetParagraphs',\n",
       " 'featDiffWordsPostText_TargetTitle',\n",
       " 'featDiffWordsTargetCaptions_TargetDescription',\n",
       " 'featDiffWordsTargetCaptions_TargetKeywords',\n",
       " 'featDiffWordsTargetCaptions_TargetParagraphs',\n",
       " 'featDiffWordsTargetCaptions_TargetTitle',\n",
       " 'featDiffWordsTargetDescription_TargetKeywords',\n",
       " 'featDiffWordsTargetDescription_TargetParagraphs',\n",
       " 'featDiffWordsTargetDescription_TargetTitle',\n",
       " 'featDiffWordsTargetKeywords_TargetParagraphs',\n",
       " 'featDiffWordsTargetKeywords_TargetTitle',\n",
       " 'featDiffWordsTargetParagraphs_TargetTitle',\n",
       " 'featRatioWordsPostText_TargetCaptions',\n",
       " 'featRatioWordsPostText_TargetDescription',\n",
       " 'featRatioWordsPostText_TargetKeywords',\n",
       " 'featRatioWordsPostText_TargetParagraphs',\n",
       " 'featRatioWordsPostText_TargetTitle',\n",
       " 'featRatioWordsTargetCaptions_TargetDescription',\n",
       " 'featRatioWordsTargetCaptions_TargetKeywords',\n",
       " 'featRatioWordsTargetCaptions_TargetParagraphs',\n",
       " 'featRatioWordsTargetCaptions_TargetTitle',\n",
       " 'featRatioWordsTargetDescription_TargetKeywords',\n",
       " 'featRatioWordsTargetDescription_TargetParagraphs',\n",
       " 'featRatioWordsTargetDescription_TargetTitle',\n",
       " 'featRatioWordsTargetKeywords_TargetParagraphs',\n",
       " 'featRatioWordsTargetKeywords_TargetTitle',\n",
       " 'featRatioWordsTargetParagraphs_TargetTitle',\n",
       " 'featNumFormalWordsPostText',\n",
       " 'featNumFormalWordsTargetTitle',\n",
       " 'featNumFormalWordsTargetDescription',\n",
       " 'featNumFormalWordsTargetKeywords',\n",
       " 'featNumFormalWordsTargetCaptions',\n",
       " 'featNumFormalWordsTargetParagraphs',\n",
       " 'featNumInformalWordsPostText',\n",
       " 'featNumInformalWordsTargetTitle',\n",
       " 'featNumInformalWordsTargetDescription',\n",
       " 'featNumInformalWordsTargetKeywords',\n",
       " 'featNumInformalWordsTargetCaptions',\n",
       " 'featNumInformalWordsTargetParagraphs',\n",
       " 'featPercentFormalWordsPostText',\n",
       " 'featPercentFormalWordsTargetCaptions',\n",
       " 'featPercentFormalWordsTargetDescription',\n",
       " 'featPercentFormalWordsTargetKeywords',\n",
       " 'featPercentFormalWordsTargetParagraphs',\n",
       " 'featPercentFormalWordsTargetTitle',\n",
       " 'featPercentInformalWordsPostText',\n",
       " 'featPercentInformalWordsTargetCaptions',\n",
       " 'featPercentInformalWordsTargetDescription',\n",
       " 'featPercentInformalWordsTargetKeywords',\n",
       " 'featPercentInformalWordsTargetParagraphs',\n",
       " 'featPercentInformalWordsTargetTitle',\n",
       " 'featCountPOS_DT',\n",
       " 'featCountPOS_IN',\n",
       " 'featCountPOS_JJ',\n",
       " 'featCountPOS_NN',\n",
       " 'featCountPOS_NNS',\n",
       " 'featCountPOS_NNP',\n",
       " 'featCountPOS_PRP',\n",
       " 'featCountPOS_RB',\n",
       " 'featCountPOS_RBS',\n",
       " 'featCountPOS_VBZ',\n",
       " 'featCountPOS_WP',\n",
       " 'featCountPOS_WRB',\n",
       " 'featCountPOS_DT_JJ',\n",
       " 'featCountPOS_DT_NN',\n",
       " 'featCountPOS_DT_NNS',\n",
       " 'featCountPOS_DT_NNP',\n",
       " 'featCountPOS_DT_RB',\n",
       " 'featCountPOS_DT_VBZ',\n",
       " 'featCountPOS_IN_DT',\n",
       " 'featCountPOS_IN_JJ',\n",
       " 'featCountPOS_IN_NN',\n",
       " 'featCountPOS_IN_NNS',\n",
       " 'featCountPOS_IN_NNP',\n",
       " 'featCountPOS_IN_PRP',\n",
       " 'featCountPOS_IN_RB',\n",
       " 'featCountPOS_JJ_DT',\n",
       " 'featCountPOS_JJ_IN',\n",
       " 'featCountPOS_JJ_NN',\n",
       " 'featCountPOS_JJ_NNS',\n",
       " 'featCountPOS_JJ_NNP',\n",
       " 'featCountPOS_JJ_PRP',\n",
       " 'featCountPOS_JJ_RB',\n",
       " 'featCountPOS_JJ_VBZ',\n",
       " 'featCountPOS_NN_DT',\n",
       " 'featCountPOS_NN_IN',\n",
       " 'featCountPOS_NN_JJ',\n",
       " 'featCountPOS_NN_NNS',\n",
       " 'featCountPOS_NN_NNP',\n",
       " 'featCountPOS_NN_PRP',\n",
       " 'featCountPOS_NN_RB',\n",
       " 'featCountPOS_NN_RBS',\n",
       " 'featCountPOS_NN_VBZ',\n",
       " 'featCountPOS_NN_WP',\n",
       " 'featCountPOS_NN_WRB',\n",
       " 'featCountPOS_NNS_DT',\n",
       " 'featCountPOS_NNS_IN',\n",
       " 'featCountPOS_NNS_JJ',\n",
       " 'featCountPOS_NNS_NN',\n",
       " 'featCountPOS_NNS_NNP',\n",
       " 'featCountPOS_NNS_PRP',\n",
       " 'featCountPOS_NNS_RB',\n",
       " 'featCountPOS_NNS_VBZ',\n",
       " 'featCountPOS_NNS_WRB',\n",
       " 'featCountPOS_NNP_DT',\n",
       " 'featCountPOS_NNP_IN',\n",
       " 'featCountPOS_NNP_JJ',\n",
       " 'featCountPOS_NNP_NN',\n",
       " 'featCountPOS_NNP_NNS',\n",
       " 'featCountPOS_NNP_PRP',\n",
       " 'featCountPOS_NNP_RB',\n",
       " 'featCountPOS_NNP_VBZ',\n",
       " 'featCountPOS_NNP_WP',\n",
       " 'featCountPOS_NNP_WRB',\n",
       " 'featCountPOS_PRP_IN',\n",
       " 'featCountPOS_PRP_JJ',\n",
       " 'featCountPOS_PRP_NN',\n",
       " 'featCountPOS_PRP_NNS',\n",
       " 'featCountPOS_PRP_NNP',\n",
       " 'featCountPOS_PRP_RB',\n",
       " 'featCountPOS_PRP_VBZ',\n",
       " 'featCountPOS_RB_DT',\n",
       " 'featCountPOS_RB_IN',\n",
       " 'featCountPOS_RB_JJ',\n",
       " 'featCountPOS_RB_NN',\n",
       " 'featCountPOS_RB_NNS',\n",
       " 'featCountPOS_RB_NNP',\n",
       " 'featCountPOS_RB_VBZ',\n",
       " 'featCountPOS_RBS_NN',\n",
       " 'featCountPOS_VBZ_DT',\n",
       " 'featCountPOS_VBZ_IN',\n",
       " 'featCountPOS_VBZ_JJ',\n",
       " 'featCountPOS_VBZ_NN',\n",
       " 'featCountPOS_VBZ_NNS',\n",
       " 'featCountPOS_VBZ_NNP',\n",
       " 'featCountPOS_VBZ_PRP',\n",
       " 'featCountPOS_VBZ_RB',\n",
       " 'featCountPOS_VBZ_WP',\n",
       " 'featCountPOS_VBZ_WRB',\n",
       " 'featCountPOS_WP_JJ',\n",
       " 'featCountPOS_WP_NN',\n",
       " 'featCountPOS_WP_NNP',\n",
       " 'featCountPOS_WP_PRP',\n",
       " 'featCountPOS_WP_RB',\n",
       " 'featCountPOS_WP_VBZ',\n",
       " 'featCountPOS_WRB_JJ',\n",
       " 'featCountPOS_WRB_NN',\n",
       " 'featCountPOS_WRB_NNS',\n",
       " 'featCountPOS_WRB_NNP',\n",
       " 'featCountPOS_WRB_PRP',\n",
       " 'featCountPOS_WRB_RB',\n",
       " 'featCountPOS_WRB_VBZ',\n",
       " 'featCountPOS_DT_JJ_NN',\n",
       " 'featCountPOS_DT_JJ_NNS',\n",
       " 'featCountPOS_DT_JJ_NNP',\n",
       " 'featCountPOS_DT_JJ_RB',\n",
       " 'featCountPOS_DT_NN_IN',\n",
       " 'featCountPOS_DT_NN_JJ',\n",
       " 'featCountPOS_DT_NN_NNS',\n",
       " 'featCountPOS_DT_NN_NNP',\n",
       " 'featCountPOS_DT_NN_RB',\n",
       " 'featCountPOS_DT_NN_VBZ',\n",
       " 'featCountPOS_DT_NNS_IN',\n",
       " 'featCountPOS_DT_NNS_NN',\n",
       " 'featCountPOS_DT_NNS_NNP',\n",
       " 'featCountPOS_DT_NNS_RB',\n",
       " 'featCountPOS_DT_NNP_IN',\n",
       " 'featCountPOS_DT_NNP_JJ',\n",
       " 'featCountPOS_DT_NNP_NN',\n",
       " 'featCountPOS_DT_NNP_NNS',\n",
       " 'featCountPOS_DT_NNP_RB',\n",
       " 'featCountPOS_DT_NNP_VBZ',\n",
       " 'featCountPOS_DT_RB_JJ',\n",
       " 'featCountPOS_DT_VBZ_NN',\n",
       " 'featCountPOS_IN_DT_NNS',\n",
       " 'featCountPOS_IN_DT_NNP',\n",
       " 'featCountPOS_IN_JJ_NN',\n",
       " 'featCountPOS_IN_JJ_NNS',\n",
       " 'featCountPOS_IN_JJ_NNP',\n",
       " 'featCountPOS_IN_JJ_VBZ',\n",
       " 'featCountPOS_IN_NN_JJ',\n",
       " 'featCountPOS_IN_NN_NNS',\n",
       " 'featCountPOS_IN_NN_NNP',\n",
       " 'featCountPOS_IN_NN_RB',\n",
       " 'featCountPOS_IN_NNS_JJ',\n",
       " 'featCountPOS_IN_NNS_NN',\n",
       " 'featCountPOS_IN_NNS_NNP',\n",
       " 'featCountPOS_IN_NNS_RB',\n",
       " 'featCountPOS_IN_NNP_DT',\n",
       " 'featCountPOS_IN_NNP_JJ',\n",
       " 'featCountPOS_IN_NNP_NN',\n",
       " 'featCountPOS_IN_NNP_NNS',\n",
       " 'featCountPOS_IN_NNP_RB',\n",
       " 'featCountPOS_IN_NNP_VBZ',\n",
       " 'featCountPOS_IN_RB_JJ',\n",
       " 'featCountPOS_IN_RB_NN',\n",
       " 'featCountPOS_JJ_DT_NN',\n",
       " 'featCountPOS_JJ_DT_NNP',\n",
       " 'featCountPOS_JJ_IN_NN',\n",
       " 'featCountPOS_JJ_IN_NNS',\n",
       " 'featCountPOS_JJ_IN_NNP',\n",
       " 'featCountPOS_JJ_NN_DT',\n",
       " 'featCountPOS_JJ_NN_IN',\n",
       " 'featCountPOS_JJ_NN_NNS',\n",
       " 'featCountPOS_JJ_NN_NNP',\n",
       " 'featCountPOS_JJ_NN_PRP',\n",
       " 'featCountPOS_JJ_NN_RB',\n",
       " 'featCountPOS_JJ_NN_VBZ',\n",
       " 'featCountPOS_JJ_NN_WP',\n",
       " 'featCountPOS_JJ_NNS_DT',\n",
       " 'featCountPOS_JJ_NNS_IN',\n",
       " 'featCountPOS_JJ_NNS_NN',\n",
       " 'featCountPOS_JJ_NNS_NNP',\n",
       " 'featCountPOS_JJ_NNS_PRP',\n",
       " 'featCountPOS_JJ_NNS_RB',\n",
       " 'featCountPOS_JJ_NNS_VBZ',\n",
       " 'featCountPOS_JJ_NNS_WRB',\n",
       " 'featCountPOS_JJ_NNP_DT',\n",
       " 'featCountPOS_JJ_NNP_IN',\n",
       " 'featCountPOS_JJ_NNP_NN',\n",
       " 'featCountPOS_JJ_NNP_NNS',\n",
       " 'featCountPOS_JJ_NNP_RB',\n",
       " 'featCountPOS_JJ_NNP_VBZ',\n",
       " 'featCountPOS_JJ_RB_NN',\n",
       " 'featCountPOS_JJ_RB_NNP',\n",
       " 'featCountPOS_JJ_RB_VBZ',\n",
       " 'featCountPOS_JJ_VBZ_IN',\n",
       " 'featCountPOS_JJ_VBZ_NN',\n",
       " 'featCountPOS_JJ_VBZ_NNS',\n",
       " 'featCountPOS_JJ_VBZ_NNP',\n",
       " 'featCountPOS_NN_DT_JJ',\n",
       " 'featCountPOS_NN_DT_NNP',\n",
       " 'featCountPOS_NN_IN_JJ',\n",
       " 'featCountPOS_NN_IN_NNS',\n",
       " 'featCountPOS_NN_IN_NNP',\n",
       " 'featCountPOS_NN_IN_PRP',\n",
       " 'featCountPOS_NN_IN_RB',\n",
       " 'featCountPOS_NN_JJ_NNS',\n",
       " 'featCountPOS_NN_JJ_NNP',\n",
       " 'featCountPOS_NN_JJ_PRP',\n",
       " 'featCountPOS_NN_JJ_RB',\n",
       " 'featCountPOS_NN_JJ_VBZ',\n",
       " 'featCountPOS_NN_NNS_DT',\n",
       " 'featCountPOS_NN_NNS_IN',\n",
       " 'featCountPOS_NN_NNS_JJ',\n",
       " 'featCountPOS_NN_NNS_NNP',\n",
       " 'featCountPOS_NN_NNS_RB',\n",
       " 'featCountPOS_NN_NNS_VBZ',\n",
       " 'featCountPOS_NN_NNP_DT',\n",
       " 'featCountPOS_NN_NNP_IN',\n",
       " 'featCountPOS_NN_NNP_JJ',\n",
       " 'featCountPOS_NN_NNP_NNS',\n",
       " 'featCountPOS_NN_NNP_PRP',\n",
       " 'featCountPOS_NN_NNP_RB',\n",
       " 'featCountPOS_NN_NNP_VBZ',\n",
       " 'featCountPOS_NN_NNP_WP',\n",
       " 'featCountPOS_NN_PRP_RB',\n",
       " 'featCountPOS_NN_PRP_VBZ',\n",
       " 'featCountPOS_NN_RB_DT',\n",
       " 'featCountPOS_NN_RB_IN',\n",
       " 'featCountPOS_NN_RB_JJ',\n",
       " 'featCountPOS_NN_RB_NNS',\n",
       " 'featCountPOS_NN_RB_NNP',\n",
       " 'featCountPOS_NN_RB_VBZ',\n",
       " 'featCountPOS_NN_VBZ_DT',\n",
       " 'featCountPOS_NN_VBZ_IN',\n",
       " 'featCountPOS_NN_VBZ_JJ',\n",
       " 'featCountPOS_NN_VBZ_NNS',\n",
       " 'featCountPOS_NN_VBZ_NNP',\n",
       " 'featCountPOS_NN_VBZ_RB',\n",
       " 'featCountPOS_NN_WP_VBZ',\n",
       " 'featCountPOS_NN_WRB_NNP',\n",
       " 'featCountPOS_NN_WRB_VBZ',\n",
       " 'featCountPOS_NNS_DT_JJ',\n",
       " 'featCountPOS_NNS_DT_NNP',\n",
       " 'featCountPOS_NNS_IN_JJ',\n",
       " 'featCountPOS_NNS_IN_NN',\n",
       " 'featCountPOS_NNS_IN_NNP',\n",
       " 'featCountPOS_NNS_JJ_IN',\n",
       " 'featCountPOS_NNS_JJ_NN',\n",
       " 'featCountPOS_NNS_JJ_NNP',\n",
       " 'featCountPOS_NNS_JJ_RB',\n",
       " 'featCountPOS_NNS_NN_IN',\n",
       " 'featCountPOS_NNS_NN_JJ',\n",
       " 'featCountPOS_NNS_NN_NNP',\n",
       " 'featCountPOS_NNS_NN_RB',\n",
       " 'featCountPOS_NNS_NN_VBZ',\n",
       " 'featCountPOS_NNS_NNP_DT',\n",
       " 'featCountPOS_NNS_NNP_IN',\n",
       " 'featCountPOS_NNS_NNP_JJ',\n",
       " 'featCountPOS_NNS_NNP_NN',\n",
       " 'featCountPOS_NNS_NNP_RB',\n",
       " 'featCountPOS_NNS_NNP_VBZ',\n",
       " 'featCountPOS_NNS_NNP_WP',\n",
       " 'featCountPOS_NNS_PRP_JJ',\n",
       " 'featCountPOS_NNS_RB_JJ',\n",
       " 'featCountPOS_NNS_RB_NN',\n",
       " 'featCountPOS_NNS_RB_NNP',\n",
       " 'featCountPOS_NNS_RB_VBZ',\n",
       " 'featCountPOS_NNS_VBZ_IN',\n",
       " 'featCountPOS_NNS_VBZ_JJ',\n",
       " 'featCountPOS_NNS_VBZ_NN',\n",
       " 'featCountPOS_NNS_VBZ_NNP',\n",
       " 'featCountPOS_NNS_WRB_JJ',\n",
       " 'featCountPOS_NNP_DT_JJ',\n",
       " 'featCountPOS_NNP_DT_NN',\n",
       " 'featCountPOS_NNP_DT_NNS',\n",
       " 'featCountPOS_NNP_DT_VBZ',\n",
       " 'featCountPOS_NNP_IN_DT',\n",
       " 'featCountPOS_NNP_IN_JJ',\n",
       " 'featCountPOS_NNP_IN_NN',\n",
       " 'featCountPOS_NNP_IN_NNS',\n",
       " 'featCountPOS_NNP_IN_RB',\n",
       " 'featCountPOS_NNP_JJ_IN',\n",
       " 'featCountPOS_NNP_JJ_NN',\n",
       " 'featCountPOS_NNP_JJ_NNS',\n",
       " 'featCountPOS_NNP_JJ_RB',\n",
       " 'featCountPOS_NNP_JJ_VBZ',\n",
       " 'featCountPOS_NNP_NN_DT',\n",
       " 'featCountPOS_NNP_NN_IN',\n",
       " 'featCountPOS_NNP_NN_JJ',\n",
       " 'featCountPOS_NNP_NN_NNS',\n",
       " 'featCountPOS_NNP_NN_PRP',\n",
       " 'featCountPOS_NNP_NN_RB',\n",
       " 'featCountPOS_NNP_NN_VBZ',\n",
       " 'featCountPOS_NNP_NN_WP',\n",
       " 'featCountPOS_NNP_NN_WRB',\n",
       " 'featCountPOS_NNP_NNS_DT',\n",
       " 'featCountPOS_NNP_NNS_IN',\n",
       " 'featCountPOS_NNP_NNS_JJ',\n",
       " 'featCountPOS_NNP_NNS_NN',\n",
       " 'featCountPOS_NNP_NNS_PRP',\n",
       " 'featCountPOS_NNP_NNS_RB',\n",
       " 'featCountPOS_NNP_NNS_VBZ',\n",
       " 'featCountPOS_NNP_PRP_NN',\n",
       " 'featCountPOS_NNP_PRP_RB',\n",
       " 'featCountPOS_NNP_PRP_VBZ',\n",
       " 'featCountPOS_NNP_RB_DT',\n",
       " 'featCountPOS_NNP_RB_JJ',\n",
       " 'featCountPOS_NNP_RB_NN',\n",
       " 'featCountPOS_NNP_RB_NNS',\n",
       " 'featCountPOS_NNP_RB_VBZ',\n",
       " 'featCountPOS_NNP_VBZ_DT',\n",
       " 'featCountPOS_NNP_VBZ_IN',\n",
       " 'featCountPOS_NNP_VBZ_JJ',\n",
       " 'featCountPOS_NNP_VBZ_NN',\n",
       " 'featCountPOS_NNP_VBZ_NNS',\n",
       " 'featCountPOS_NNP_VBZ_PRP',\n",
       " 'featCountPOS_NNP_VBZ_RB',\n",
       " 'featCountPOS_NNP_VBZ_WP',\n",
       " 'featCountPOS_NNP_VBZ_WRB',\n",
       " 'featCountPOS_NNP_WP_JJ',\n",
       " 'featCountPOS_NNP_WP_NN',\n",
       " 'featCountPOS_NNP_WP_PRP',\n",
       " 'featCountPOS_NNP_WP_RB',\n",
       " 'featCountPOS_NNP_WP_VBZ',\n",
       " 'featCountPOS_NNP_WRB_JJ',\n",
       " 'featCountPOS_NNP_WRB_NNS',\n",
       " 'featCountPOS_NNP_WRB_PRP',\n",
       " 'featCountPOS_PRP_IN_NNS',\n",
       " 'featCountPOS_PRP_JJ_NN',\n",
       " 'featCountPOS_PRP_JJ_NNP',\n",
       " 'featCountPOS_PRP_NN_JJ',\n",
       " 'featCountPOS_PRP_NN_NNP',\n",
       " 'featCountPOS_PRP_NNP_JJ',\n",
       " 'featCountPOS_PRP_NNP_VBZ',\n",
       " 'featCountPOS_PRP_RB_IN',\n",
       " 'featCountPOS_PRP_VBZ_IN',\n",
       " 'featCountPOS_PRP_VBZ_JJ',\n",
       " 'featCountPOS_PRP_VBZ_NN',\n",
       " 'featCountPOS_PRP_VBZ_NNP',\n",
       " 'featCountPOS_PRP_VBZ_RB',\n",
       " 'featCountPOS_RB_DT_NN',\n",
       " 'featCountPOS_RB_DT_NNP',\n",
       " 'featCountPOS_RB_IN_DT',\n",
       " 'featCountPOS_RB_IN_JJ',\n",
       " 'featCountPOS_RB_IN_NN',\n",
       " 'featCountPOS_RB_IN_NNP',\n",
       " 'featCountPOS_RB_JJ_NN',\n",
       " 'featCountPOS_RB_JJ_NNS',\n",
       " 'featCountPOS_RB_JJ_NNP',\n",
       " 'featCountPOS_RB_JJ_PRP',\n",
       " 'featCountPOS_RB_NN_NNS',\n",
       " 'featCountPOS_RB_NN_NNP',\n",
       " 'featCountPOS_RB_NN_PRP',\n",
       " 'featCountPOS_RB_NNS_IN',\n",
       " 'featCountPOS_RB_NNS_JJ',\n",
       " 'featCountPOS_RB_NNP_DT',\n",
       " 'featCountPOS_RB_NNP_JJ',\n",
       " 'featCountPOS_RB_NNP_NN',\n",
       " 'featCountPOS_RB_NNP_NNS',\n",
       " 'featCountPOS_RB_NNP_VBZ',\n",
       " 'featCountPOS_RB_VBZ_IN',\n",
       " 'featCountPOS_RB_VBZ_JJ',\n",
       " 'featCountPOS_RB_VBZ_NN',\n",
       " 'featCountPOS_RB_VBZ_NNS',\n",
       " 'featCountPOS_RB_VBZ_NNP',\n",
       " 'featCountPOS_RB_VBZ_WP',\n",
       " 'featCountPOS_VBZ_DT_JJ',\n",
       " 'featCountPOS_VBZ_DT_NN',\n",
       " 'featCountPOS_VBZ_DT_NNS',\n",
       " 'featCountPOS_VBZ_DT_NNP',\n",
       " 'featCountPOS_VBZ_IN_JJ',\n",
       " 'featCountPOS_VBZ_IN_NN',\n",
       " 'featCountPOS_VBZ_IN_NNS',\n",
       " 'featCountPOS_VBZ_IN_NNP',\n",
       " 'featCountPOS_VBZ_JJ_IN',\n",
       " 'featCountPOS_VBZ_JJ_NN',\n",
       " 'featCountPOS_VBZ_JJ_NNS',\n",
       " 'featCountPOS_VBZ_JJ_NNP',\n",
       " 'featCountPOS_VBZ_JJ_RB',\n",
       " 'featCountPOS_VBZ_NN_JJ',\n",
       " 'featCountPOS_VBZ_NN_NNS',\n",
       " 'featCountPOS_VBZ_NN_NNP',\n",
       " 'featCountPOS_VBZ_NN_RB',\n",
       " 'featCountPOS_VBZ_NN_RBS',\n",
       " 'featCountPOS_VBZ_NNS_IN',\n",
       " 'featCountPOS_VBZ_NNS_JJ',\n",
       " 'featCountPOS_VBZ_NNS_NN',\n",
       " 'featCountPOS_VBZ_NNS_NNP',\n",
       " 'featCountPOS_VBZ_NNP_IN',\n",
       " 'featCountPOS_VBZ_NNP_JJ',\n",
       " 'featCountPOS_VBZ_NNP_NN',\n",
       " 'featCountPOS_VBZ_NNP_NNS',\n",
       " 'featCountPOS_VBZ_NNP_RB',\n",
       " 'featCountPOS_VBZ_PRP_NNP',\n",
       " 'featCountPOS_VBZ_RB_IN',\n",
       " 'featCountPOS_VBZ_RB_JJ',\n",
       " 'featCountPOS_VBZ_RB_NN',\n",
       " 'featCountPOS_VBZ_RB_NNS',\n",
       " 'featCountPOS_VBZ_RB_NNP',\n",
       " 'featCountPOS_VBZ_WP_NNP',\n",
       " 'featCountPOS_VBZ_WRB_NNS',\n",
       " 'featCountPOS_WP_NN_IN',\n",
       " 'featCountPOS_WP_NN_NNS',\n",
       " 'featCountPOS_WP_NNP_VBZ',\n",
       " 'featCountPOS_WP_VBZ_NNS',\n",
       " 'featCountPOS_WRB_JJ_NN',\n",
       " 'featCountPOS_WRB_JJ_NNS',\n",
       " 'featCountPOS_WRB_JJ_NNP',\n",
       " 'featCountPOS_WRB_JJ_VBZ',\n",
       " 'featCountPOS_WRB_NN_NNP',\n",
       " 'featCountPOS_WRB_NNS_JJ',\n",
       " 'featCountPOS_WRB_NNS_RB',\n",
       " 'featCountPOS_WRB_NNP_JJ',\n",
       " 'featCountPOS_WRB_NNP_NN',\n",
       " 'featCountPOS_WRB_NNP_NNS',\n",
       " 'featCountPOS_WRB_NNP_VBZ',\n",
       " 'featCountPOS_WRB_RB_NN',\n",
       " 'featCountPOS_WRB_VBZ_NNS',\n",
       " 'featIsNEPresent',\n",
       " 'featSentiment',\n",
       " 'featSimilarityPostTextTargetTitle',\n",
       " 'featSimilarityPostTextTargetParagraphs',\n",
       " 'featSimilarityPostTextTargetKeywords']"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_large, X_test_large, y_train_large, y_test_large = None,None,None,None\n",
    "names = list(data)\n",
    "names_large = list(data_large)\n",
    "not_in = list(set(names) ^ set(names_large))\n",
    "size_set = \"large\"\n",
    "if(size_set == \"small\"):   \n",
    "    dataset= \"feature_set_small.csv\"\n",
    "    feature = \"labels_set_small.csv\"  \n",
    "else:\n",
    "    dataset = \"feature_set_large.csv\"\n",
    "    feature = \"labels_set_large.csv\"\n",
    "\n",
    "data_large = pd.read_csv(dataset).fillna(0)\n",
    "data_large = data_large[data_large.columns.intersection(names)]\n",
    "data_large = pd.concat([data_large,pd.DataFrame(0,index=np.arange(len(data_large)),columns=list(not_in))],axis=1)\n",
    "\n",
    "labels_large = pd.read_csv(feature)\n",
    "labels_large = labels_large[['truthClass','truthMean']]\n",
    "X_test_large, y_test_large = data_large.values, labels_large.values[:,0]\n",
    "\n",
    "ratio = float(np.sum(y_test_large == 'no-clickbait')) / np.sum(y_test_large == 'clickbait')\n",
    "classifier_names = [\"AdaBoost\",\"XGBoost\",\"Naive Bayes\", \"KNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>featNumCharPostText</th>\n",
       "      <th>featNumCharTargetTitle</th>\n",
       "      <th>featNumCharTargetDescription</th>\n",
       "      <th>featNumCharTargetKeywords</th>\n",
       "      <th>featNumCharTargetCaptions</th>\n",
       "      <th>featNumCharTargetParagraphs</th>\n",
       "      <th>featDiffCharPostText_TargetCaptions</th>\n",
       "      <th>featDiffCharPostText_TargetDescription</th>\n",
       "      <th>...</th>\n",
       "      <th>featCountPOS_NNS_NNP_WP</th>\n",
       "      <th>featCountPOS_NNP_WP_JJ</th>\n",
       "      <th>featCountPOS_PRP_JJ_NNP</th>\n",
       "      <th>featCountPOS_WRB_RB_NN</th>\n",
       "      <th>featCountPOS_VBZ_WRB_NNS</th>\n",
       "      <th>featCountPOS_VBZ_RB_IN</th>\n",
       "      <th>featCountPOS_NN_WRB_VBZ</th>\n",
       "      <th>featCountPOS_NNP_VBZ_WRB</th>\n",
       "      <th>featCountPOS_RB_VBZ_WP</th>\n",
       "      <th>featCountPOS_PRP_IN_NNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19508</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19509</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19510</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19511</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19512</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19513</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19514</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19515</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19516</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19517</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19520</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19521</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19522</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19523</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19524</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19525</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19526</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19527</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19528</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19529</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19530</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19531</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19532</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19533</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19534</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19535</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19536</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19537</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19538 rows  502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id  featNumCharPostText  featNumCharTargetTitle  \\\n",
       "0           False  False                False                   False   \n",
       "1           False  False                False                   False   \n",
       "2           False  False                False                   False   \n",
       "3           False  False                False                   False   \n",
       "4           False  False                False                   False   \n",
       "5           False  False                False                   False   \n",
       "6           False  False                False                   False   \n",
       "7           False  False                False                   False   \n",
       "8           False  False                False                   False   \n",
       "9           False  False                False                   False   \n",
       "10          False  False                False                   False   \n",
       "11          False  False                False                   False   \n",
       "12          False  False                False                   False   \n",
       "13          False  False                False                   False   \n",
       "14          False  False                False                   False   \n",
       "15          False  False                False                   False   \n",
       "16          False  False                False                   False   \n",
       "17          False  False                False                   False   \n",
       "18          False  False                False                   False   \n",
       "19          False  False                False                   False   \n",
       "20          False  False                False                   False   \n",
       "21          False  False                False                   False   \n",
       "22          False  False                False                   False   \n",
       "23          False  False                False                   False   \n",
       "24          False  False                False                   False   \n",
       "25          False  False                False                   False   \n",
       "26          False  False                False                   False   \n",
       "27          False  False                False                   False   \n",
       "28          False  False                False                   False   \n",
       "29          False  False                False                   False   \n",
       "...           ...    ...                  ...                     ...   \n",
       "19508       False  False                False                   False   \n",
       "19509       False  False                False                   False   \n",
       "19510       False  False                False                   False   \n",
       "19511       False  False                False                   False   \n",
       "19512       False  False                False                   False   \n",
       "19513       False  False                False                   False   \n",
       "19514       False  False                False                   False   \n",
       "19515       False  False                False                   False   \n",
       "19516       False  False                False                   False   \n",
       "19517       False  False                False                   False   \n",
       "19518       False  False                False                   False   \n",
       "19519       False  False                False                   False   \n",
       "19520       False  False                False                   False   \n",
       "19521       False  False                False                   False   \n",
       "19522       False  False                False                   False   \n",
       "19523       False  False                False                   False   \n",
       "19524       False  False                False                   False   \n",
       "19525       False  False                False                   False   \n",
       "19526       False  False                False                   False   \n",
       "19527       False  False                False                   False   \n",
       "19528       False  False                False                   False   \n",
       "19529       False  False                False                   False   \n",
       "19530       False  False                False                   False   \n",
       "19531       False  False                False                   False   \n",
       "19532       False  False                False                   False   \n",
       "19533       False  False                False                   False   \n",
       "19534       False  False                False                   False   \n",
       "19535       False  False                False                   False   \n",
       "19536       False  False                False                   False   \n",
       "19537       False  False                False                   False   \n",
       "\n",
       "       featNumCharTargetDescription  featNumCharTargetKeywords  \\\n",
       "0                             False                      False   \n",
       "1                             False                      False   \n",
       "2                             False                      False   \n",
       "3                             False                      False   \n",
       "4                             False                      False   \n",
       "5                             False                      False   \n",
       "6                             False                      False   \n",
       "7                             False                      False   \n",
       "8                             False                      False   \n",
       "9                             False                      False   \n",
       "10                            False                      False   \n",
       "11                            False                      False   \n",
       "12                            False                      False   \n",
       "13                            False                      False   \n",
       "14                            False                      False   \n",
       "15                            False                      False   \n",
       "16                            False                      False   \n",
       "17                            False                      False   \n",
       "18                            False                      False   \n",
       "19                            False                      False   \n",
       "20                            False                      False   \n",
       "21                            False                      False   \n",
       "22                            False                      False   \n",
       "23                            False                      False   \n",
       "24                            False                      False   \n",
       "25                            False                      False   \n",
       "26                            False                      False   \n",
       "27                            False                      False   \n",
       "28                            False                      False   \n",
       "29                            False                      False   \n",
       "...                             ...                        ...   \n",
       "19508                         False                      False   \n",
       "19509                         False                      False   \n",
       "19510                         False                      False   \n",
       "19511                         False                      False   \n",
       "19512                         False                      False   \n",
       "19513                         False                      False   \n",
       "19514                         False                      False   \n",
       "19515                         False                      False   \n",
       "19516                         False                      False   \n",
       "19517                         False                      False   \n",
       "19518                         False                      False   \n",
       "19519                         False                      False   \n",
       "19520                         False                      False   \n",
       "19521                         False                      False   \n",
       "19522                         False                      False   \n",
       "19523                         False                      False   \n",
       "19524                         False                      False   \n",
       "19525                         False                      False   \n",
       "19526                         False                      False   \n",
       "19527                         False                      False   \n",
       "19528                         False                      False   \n",
       "19529                         False                      False   \n",
       "19530                         False                      False   \n",
       "19531                         False                      False   \n",
       "19532                         False                      False   \n",
       "19533                         False                      False   \n",
       "19534                         False                      False   \n",
       "19535                         False                      False   \n",
       "19536                         False                      False   \n",
       "19537                         False                      False   \n",
       "\n",
       "       featNumCharTargetCaptions  featNumCharTargetParagraphs  \\\n",
       "0                          False                        False   \n",
       "1                          False                        False   \n",
       "2                          False                        False   \n",
       "3                          False                        False   \n",
       "4                          False                        False   \n",
       "5                          False                        False   \n",
       "6                          False                        False   \n",
       "7                          False                        False   \n",
       "8                          False                        False   \n",
       "9                          False                        False   \n",
       "10                         False                        False   \n",
       "11                         False                        False   \n",
       "12                         False                        False   \n",
       "13                         False                        False   \n",
       "14                         False                        False   \n",
       "15                         False                        False   \n",
       "16                         False                        False   \n",
       "17                         False                        False   \n",
       "18                         False                        False   \n",
       "19                         False                        False   \n",
       "20                         False                        False   \n",
       "21                         False                        False   \n",
       "22                         False                        False   \n",
       "23                         False                        False   \n",
       "24                         False                        False   \n",
       "25                         False                        False   \n",
       "26                         False                        False   \n",
       "27                         False                        False   \n",
       "28                         False                        False   \n",
       "29                         False                        False   \n",
       "...                          ...                          ...   \n",
       "19508                      False                        False   \n",
       "19509                      False                        False   \n",
       "19510                      False                        False   \n",
       "19511                      False                        False   \n",
       "19512                      False                        False   \n",
       "19513                      False                        False   \n",
       "19514                      False                        False   \n",
       "19515                      False                        False   \n",
       "19516                      False                        False   \n",
       "19517                      False                        False   \n",
       "19518                      False                        False   \n",
       "19519                      False                        False   \n",
       "19520                      False                        False   \n",
       "19521                      False                        False   \n",
       "19522                      False                        False   \n",
       "19523                      False                        False   \n",
       "19524                      False                        False   \n",
       "19525                      False                        False   \n",
       "19526                      False                        False   \n",
       "19527                      False                        False   \n",
       "19528                      False                        False   \n",
       "19529                      False                        False   \n",
       "19530                      False                        False   \n",
       "19531                      False                        False   \n",
       "19532                      False                        False   \n",
       "19533                      False                        False   \n",
       "19534                      False                        False   \n",
       "19535                      False                        False   \n",
       "19536                      False                        False   \n",
       "19537                      False                        False   \n",
       "\n",
       "       featDiffCharPostText_TargetCaptions  \\\n",
       "0                                    False   \n",
       "1                                    False   \n",
       "2                                    False   \n",
       "3                                    False   \n",
       "4                                    False   \n",
       "5                                    False   \n",
       "6                                    False   \n",
       "7                                    False   \n",
       "8                                    False   \n",
       "9                                    False   \n",
       "10                                   False   \n",
       "11                                   False   \n",
       "12                                   False   \n",
       "13                                   False   \n",
       "14                                   False   \n",
       "15                                   False   \n",
       "16                                   False   \n",
       "17                                   False   \n",
       "18                                   False   \n",
       "19                                   False   \n",
       "20                                   False   \n",
       "21                                   False   \n",
       "22                                   False   \n",
       "23                                   False   \n",
       "24                                   False   \n",
       "25                                   False   \n",
       "26                                   False   \n",
       "27                                   False   \n",
       "28                                   False   \n",
       "29                                   False   \n",
       "...                                    ...   \n",
       "19508                                False   \n",
       "19509                                False   \n",
       "19510                                False   \n",
       "19511                                False   \n",
       "19512                                False   \n",
       "19513                                False   \n",
       "19514                                False   \n",
       "19515                                False   \n",
       "19516                                False   \n",
       "19517                                False   \n",
       "19518                                False   \n",
       "19519                                False   \n",
       "19520                                False   \n",
       "19521                                False   \n",
       "19522                                False   \n",
       "19523                                False   \n",
       "19524                                False   \n",
       "19525                                False   \n",
       "19526                                False   \n",
       "19527                                False   \n",
       "19528                                False   \n",
       "19529                                False   \n",
       "19530                                False   \n",
       "19531                                False   \n",
       "19532                                False   \n",
       "19533                                False   \n",
       "19534                                False   \n",
       "19535                                False   \n",
       "19536                                False   \n",
       "19537                                False   \n",
       "\n",
       "       featDiffCharPostText_TargetDescription           ...             \\\n",
       "0                                       False           ...              \n",
       "1                                       False           ...              \n",
       "2                                       False           ...              \n",
       "3                                       False           ...              \n",
       "4                                       False           ...              \n",
       "5                                       False           ...              \n",
       "6                                       False           ...              \n",
       "7                                       False           ...              \n",
       "8                                       False           ...              \n",
       "9                                       False           ...              \n",
       "10                                      False           ...              \n",
       "11                                      False           ...              \n",
       "12                                      False           ...              \n",
       "13                                      False           ...              \n",
       "14                                      False           ...              \n",
       "15                                      False           ...              \n",
       "16                                      False           ...              \n",
       "17                                      False           ...              \n",
       "18                                      False           ...              \n",
       "19                                      False           ...              \n",
       "20                                      False           ...              \n",
       "21                                      False           ...              \n",
       "22                                      False           ...              \n",
       "23                                      False           ...              \n",
       "24                                      False           ...              \n",
       "25                                      False           ...              \n",
       "26                                      False           ...              \n",
       "27                                      False           ...              \n",
       "28                                      False           ...              \n",
       "29                                      False           ...              \n",
       "...                                       ...           ...              \n",
       "19508                                   False           ...              \n",
       "19509                                   False           ...              \n",
       "19510                                   False           ...              \n",
       "19511                                   False           ...              \n",
       "19512                                   False           ...              \n",
       "19513                                   False           ...              \n",
       "19514                                   False           ...              \n",
       "19515                                   False           ...              \n",
       "19516                                   False           ...              \n",
       "19517                                   False           ...              \n",
       "19518                                   False           ...              \n",
       "19519                                   False           ...              \n",
       "19520                                   False           ...              \n",
       "19521                                   False           ...              \n",
       "19522                                   False           ...              \n",
       "19523                                   False           ...              \n",
       "19524                                   False           ...              \n",
       "19525                                   False           ...              \n",
       "19526                                   False           ...              \n",
       "19527                                   False           ...              \n",
       "19528                                   False           ...              \n",
       "19529                                   False           ...              \n",
       "19530                                   False           ...              \n",
       "19531                                   False           ...              \n",
       "19532                                   False           ...              \n",
       "19533                                   False           ...              \n",
       "19534                                   False           ...              \n",
       "19535                                   False           ...              \n",
       "19536                                   False           ...              \n",
       "19537                                   False           ...              \n",
       "\n",
       "       featCountPOS_NNS_NNP_WP  featCountPOS_NNP_WP_JJ  \\\n",
       "0                        False                   False   \n",
       "1                        False                   False   \n",
       "2                        False                   False   \n",
       "3                        False                   False   \n",
       "4                        False                   False   \n",
       "5                        False                   False   \n",
       "6                        False                   False   \n",
       "7                        False                   False   \n",
       "8                        False                   False   \n",
       "9                        False                   False   \n",
       "10                       False                   False   \n",
       "11                       False                   False   \n",
       "12                       False                   False   \n",
       "13                       False                   False   \n",
       "14                       False                   False   \n",
       "15                       False                   False   \n",
       "16                       False                   False   \n",
       "17                       False                   False   \n",
       "18                       False                   False   \n",
       "19                       False                   False   \n",
       "20                       False                   False   \n",
       "21                       False                   False   \n",
       "22                       False                   False   \n",
       "23                       False                   False   \n",
       "24                       False                   False   \n",
       "25                       False                   False   \n",
       "26                       False                   False   \n",
       "27                       False                   False   \n",
       "28                       False                   False   \n",
       "29                       False                   False   \n",
       "...                        ...                     ...   \n",
       "19508                    False                   False   \n",
       "19509                    False                   False   \n",
       "19510                    False                   False   \n",
       "19511                    False                   False   \n",
       "19512                    False                   False   \n",
       "19513                    False                   False   \n",
       "19514                    False                   False   \n",
       "19515                    False                   False   \n",
       "19516                    False                   False   \n",
       "19517                    False                   False   \n",
       "19518                    False                   False   \n",
       "19519                    False                   False   \n",
       "19520                    False                   False   \n",
       "19521                    False                   False   \n",
       "19522                    False                   False   \n",
       "19523                    False                   False   \n",
       "19524                    False                   False   \n",
       "19525                    False                   False   \n",
       "19526                    False                   False   \n",
       "19527                    False                   False   \n",
       "19528                    False                   False   \n",
       "19529                    False                   False   \n",
       "19530                    False                   False   \n",
       "19531                    False                   False   \n",
       "19532                    False                   False   \n",
       "19533                    False                   False   \n",
       "19534                    False                   False   \n",
       "19535                    False                   False   \n",
       "19536                    False                   False   \n",
       "19537                    False                   False   \n",
       "\n",
       "       featCountPOS_PRP_JJ_NNP  featCountPOS_WRB_RB_NN  \\\n",
       "0                        False                   False   \n",
       "1                        False                   False   \n",
       "2                        False                   False   \n",
       "3                        False                   False   \n",
       "4                        False                   False   \n",
       "5                        False                   False   \n",
       "6                        False                   False   \n",
       "7                        False                   False   \n",
       "8                        False                   False   \n",
       "9                        False                   False   \n",
       "10                       False                   False   \n",
       "11                       False                   False   \n",
       "12                       False                   False   \n",
       "13                       False                   False   \n",
       "14                       False                   False   \n",
       "15                       False                   False   \n",
       "16                       False                   False   \n",
       "17                       False                   False   \n",
       "18                       False                   False   \n",
       "19                       False                   False   \n",
       "20                       False                   False   \n",
       "21                       False                   False   \n",
       "22                       False                   False   \n",
       "23                       False                   False   \n",
       "24                       False                   False   \n",
       "25                       False                   False   \n",
       "26                       False                   False   \n",
       "27                       False                   False   \n",
       "28                       False                   False   \n",
       "29                       False                   False   \n",
       "...                        ...                     ...   \n",
       "19508                    False                   False   \n",
       "19509                    False                   False   \n",
       "19510                    False                   False   \n",
       "19511                    False                   False   \n",
       "19512                    False                   False   \n",
       "19513                    False                   False   \n",
       "19514                    False                   False   \n",
       "19515                    False                   False   \n",
       "19516                    False                   False   \n",
       "19517                    False                   False   \n",
       "19518                    False                   False   \n",
       "19519                    False                   False   \n",
       "19520                    False                   False   \n",
       "19521                    False                   False   \n",
       "19522                    False                   False   \n",
       "19523                    False                   False   \n",
       "19524                    False                   False   \n",
       "19525                    False                   False   \n",
       "19526                    False                   False   \n",
       "19527                    False                   False   \n",
       "19528                    False                   False   \n",
       "19529                    False                   False   \n",
       "19530                    False                   False   \n",
       "19531                    False                   False   \n",
       "19532                    False                   False   \n",
       "19533                    False                   False   \n",
       "19534                    False                   False   \n",
       "19535                    False                   False   \n",
       "19536                    False                   False   \n",
       "19537                    False                   False   \n",
       "\n",
       "       featCountPOS_VBZ_WRB_NNS  featCountPOS_VBZ_RB_IN  \\\n",
       "0                         False                   False   \n",
       "1                         False                   False   \n",
       "2                         False                   False   \n",
       "3                         False                   False   \n",
       "4                         False                   False   \n",
       "5                         False                   False   \n",
       "6                         False                   False   \n",
       "7                         False                   False   \n",
       "8                         False                   False   \n",
       "9                         False                   False   \n",
       "10                        False                   False   \n",
       "11                        False                   False   \n",
       "12                        False                   False   \n",
       "13                        False                   False   \n",
       "14                        False                   False   \n",
       "15                        False                   False   \n",
       "16                        False                   False   \n",
       "17                        False                   False   \n",
       "18                        False                   False   \n",
       "19                        False                   False   \n",
       "20                        False                   False   \n",
       "21                        False                   False   \n",
       "22                        False                   False   \n",
       "23                        False                   False   \n",
       "24                        False                   False   \n",
       "25                        False                   False   \n",
       "26                        False                   False   \n",
       "27                        False                   False   \n",
       "28                        False                   False   \n",
       "29                        False                   False   \n",
       "...                         ...                     ...   \n",
       "19508                     False                   False   \n",
       "19509                     False                   False   \n",
       "19510                     False                   False   \n",
       "19511                     False                   False   \n",
       "19512                     False                   False   \n",
       "19513                     False                   False   \n",
       "19514                     False                   False   \n",
       "19515                     False                   False   \n",
       "19516                     False                   False   \n",
       "19517                     False                   False   \n",
       "19518                     False                   False   \n",
       "19519                     False                   False   \n",
       "19520                     False                   False   \n",
       "19521                     False                   False   \n",
       "19522                     False                   False   \n",
       "19523                     False                   False   \n",
       "19524                     False                   False   \n",
       "19525                     False                   False   \n",
       "19526                     False                   False   \n",
       "19527                     False                   False   \n",
       "19528                     False                   False   \n",
       "19529                     False                   False   \n",
       "19530                     False                   False   \n",
       "19531                     False                   False   \n",
       "19532                     False                   False   \n",
       "19533                     False                   False   \n",
       "19534                     False                   False   \n",
       "19535                     False                   False   \n",
       "19536                     False                   False   \n",
       "19537                     False                   False   \n",
       "\n",
       "       featCountPOS_NN_WRB_VBZ  featCountPOS_NNP_VBZ_WRB  \\\n",
       "0                        False                     False   \n",
       "1                        False                     False   \n",
       "2                        False                     False   \n",
       "3                        False                     False   \n",
       "4                        False                     False   \n",
       "5                        False                     False   \n",
       "6                        False                     False   \n",
       "7                        False                     False   \n",
       "8                        False                     False   \n",
       "9                        False                     False   \n",
       "10                       False                     False   \n",
       "11                       False                     False   \n",
       "12                       False                     False   \n",
       "13                       False                     False   \n",
       "14                       False                     False   \n",
       "15                       False                     False   \n",
       "16                       False                     False   \n",
       "17                       False                     False   \n",
       "18                       False                     False   \n",
       "19                       False                     False   \n",
       "20                       False                     False   \n",
       "21                       False                     False   \n",
       "22                       False                     False   \n",
       "23                       False                     False   \n",
       "24                       False                     False   \n",
       "25                       False                     False   \n",
       "26                       False                     False   \n",
       "27                       False                     False   \n",
       "28                       False                     False   \n",
       "29                       False                     False   \n",
       "...                        ...                       ...   \n",
       "19508                    False                     False   \n",
       "19509                    False                     False   \n",
       "19510                    False                     False   \n",
       "19511                    False                     False   \n",
       "19512                    False                     False   \n",
       "19513                    False                     False   \n",
       "19514                    False                     False   \n",
       "19515                    False                     False   \n",
       "19516                    False                     False   \n",
       "19517                    False                     False   \n",
       "19518                    False                     False   \n",
       "19519                    False                     False   \n",
       "19520                    False                     False   \n",
       "19521                    False                     False   \n",
       "19522                    False                     False   \n",
       "19523                    False                     False   \n",
       "19524                    False                     False   \n",
       "19525                    False                     False   \n",
       "19526                    False                     False   \n",
       "19527                    False                     False   \n",
       "19528                    False                     False   \n",
       "19529                    False                     False   \n",
       "19530                    False                     False   \n",
       "19531                    False                     False   \n",
       "19532                    False                     False   \n",
       "19533                    False                     False   \n",
       "19534                    False                     False   \n",
       "19535                    False                     False   \n",
       "19536                    False                     False   \n",
       "19537                    False                     False   \n",
       "\n",
       "       featCountPOS_RB_VBZ_WP  featCountPOS_PRP_IN_NNS  \n",
       "0                       False                    False  \n",
       "1                       False                    False  \n",
       "2                       False                    False  \n",
       "3                       False                    False  \n",
       "4                       False                    False  \n",
       "5                       False                    False  \n",
       "6                       False                    False  \n",
       "7                       False                    False  \n",
       "8                       False                    False  \n",
       "9                       False                    False  \n",
       "10                      False                    False  \n",
       "11                      False                    False  \n",
       "12                      False                    False  \n",
       "13                      False                    False  \n",
       "14                      False                    False  \n",
       "15                      False                    False  \n",
       "16                      False                    False  \n",
       "17                      False                    False  \n",
       "18                      False                    False  \n",
       "19                      False                    False  \n",
       "20                      False                    False  \n",
       "21                      False                    False  \n",
       "22                      False                    False  \n",
       "23                      False                    False  \n",
       "24                      False                    False  \n",
       "25                      False                    False  \n",
       "26                      False                    False  \n",
       "27                      False                    False  \n",
       "28                      False                    False  \n",
       "29                      False                    False  \n",
       "...                       ...                      ...  \n",
       "19508                   False                    False  \n",
       "19509                   False                    False  \n",
       "19510                   False                    False  \n",
       "19511                   False                    False  \n",
       "19512                   False                    False  \n",
       "19513                   False                    False  \n",
       "19514                   False                    False  \n",
       "19515                   False                    False  \n",
       "19516                   False                    False  \n",
       "19517                   False                    False  \n",
       "19518                   False                    False  \n",
       "19519                   False                    False  \n",
       "19520                   False                    False  \n",
       "19521                   False                    False  \n",
       "19522                   False                    False  \n",
       "19523                   False                    False  \n",
       "19524                   False                    False  \n",
       "19525                   False                    False  \n",
       "19526                   False                    False  \n",
       "19527                   False                    False  \n",
       "19528                   False                    False  \n",
       "19529                   False                    False  \n",
       "19530                   False                    False  \n",
       "19531                   False                    False  \n",
       "19532                   False                    False  \n",
       "19533                   False                    False  \n",
       "19534                   False                    False  \n",
       "19535                   False                    False  \n",
       "19536                   False                    False  \n",
       "19537                   False                    False  \n",
       "\n",
       "[19538 rows x 502 columns]"
      ]
     },
     "execution_count": 977,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_large.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "[new_xtrain,new_xtest_large] = scaling_features(X_train,X_test_large,\"standard\")\n",
    "pca = PCA(n_components = 250)\n",
    "pca.fit(new_xtrain)\n",
    "pca_x_train = pca.transform(new_xtrain)\n",
    "pca_x_test = pca.transform(new_xtest_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=40, random_state=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.26      0.01      0.01      4761\n",
      "no-clickbait       0.76      0.99      0.86     14777\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     19538\n",
      "   macro avg       0.51      0.50      0.44     19538\n",
      "weighted avg       0.64      0.75      0.65     19538\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[   29  4732]\n",
      " [   83 14694]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.430566330488751,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.25      0.16      0.20      4761\n",
      "no-clickbait       0.76      0.84      0.80     14777\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     19538\n",
      "   macro avg       0.50      0.50      0.50     19538\n",
      "weighted avg       0.63      0.68      0.65     19538\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[  780  3981]\n",
      " [ 2361 12416]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.24      1.00      0.39      4761\n",
      "no-clickbait       0.00      0.00      0.00     14777\n",
      "\n",
      "   micro avg       0.24      0.24      0.24     19538\n",
      "   macro avg       0.12      0.50      0.20     19538\n",
      "weighted avg       0.06      0.24      0.10     19538\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4761     0]\n",
      " [14777     0]]\n",
      "--------------------------\n",
      "STEP IN PIPELINE:  PCA\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='uniform'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   clickbait       0.24      0.99      0.39      4761\n",
      "no-clickbait       0.72      0.01      0.02     14777\n",
      "\n",
      "   micro avg       0.25      0.25      0.25     19538\n",
      "   macro avg       0.48      0.50      0.21     19538\n",
      "weighted avg       0.60      0.25      0.11     19538\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4694    67]\n",
      " [14605   172]]\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "report_classifiers(classifiers,pca_x_test,y_test_large,\"PCA\",0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
